{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_yxCfjvnS_I"
      },
      "source": [
        "# Deep Q Learning.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9p2ibhhp-ta"
      },
      "source": [
        "If the number of states becomes too large, tabular learning becomes unfeasible. A way to circumvent this problem is to model $Q$ as a deep neural network that receives as inputs the state and the action, and outputs the corresponding $Q$ value. The parameters of this network can then be learned via gradient descent by considering a loss that measures how far $Q$ is from satisfying Bellman's equation.\n",
        "\n",
        "This is the basic conceptual idea behind the major breakthrough made in 2013 by a team from DeepMind https://arxiv.org/abs/1312.5602, which allowed them to train a DQN (Deep Q Network) to play several Atari games; in fact, the networke learned \"by itself\" starting from the pixel data of the frames of the computer games. This remarkable breakthrough was the starting point to a spectacular series of achievements that included mastering high-complexity strategy games such as chess and Go, and most importantly, the computation of the 3D structure of proteins https://www.nobelprize.org/prizes/chemistry/2024/summary/.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AappnB_LtXdi"
      },
      "source": [
        "A naive version of the DQN algorithm allude to above, can be summarizes as follows:\n",
        "\n",
        "1. Initialize $Q(s, a)$ using a standard deep learning initialization.\n",
        "\n",
        "2. By interacting with the environment, obtain the tuple $(s, a, r, s')$.\n",
        "\n",
        "3. Compute the loss:\n",
        "\n",
        "$${\\cal L} =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " (Q(s, a) - r)^2  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " (Q(s, a) - (r + \\gamma \\max_{a'} Q(s' ,a'))^2 \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "4. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "5. Repeat from step 2 until convergence (in the sense described for tabular-$Q$ learning) is achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCs21Aj2b50h"
      },
      "source": [
        "We'll now focus on a single Atari game, the iconic Pong https://ale.farama.org/environments/pong/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PhOeajsRcnXZ",
        "outputId": "7d16ec35-c3b7-496b-f702-ac621af9973c"
      },
      "outputs": [],
      "source": [
        "# !pip install 'stable_baselines3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "MZeLCmSQb5IR",
        "outputId": "9f89fbec-7b4a-4b1c-9515-b08739f1c4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(210, 160, 3)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]]], dtype=uint8)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "import ale_py\n",
        "\n",
        "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "\n",
        "obs,_ = env.reset()\n",
        "\n",
        "print(obs.shape)\n",
        "\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSoytnibWas"
      },
      "source": [
        "Unfortunately, the previous version of DQN doesn't work very well. So we need to do perform some upgrades.\n",
        "\n",
        "- On one hand, we need to explore the environment (using random actions); on the other, we want to use the knowledge gained by the $Q$-function; this is the famous \"exploration versus exploitation dilemma\".  We will resolve this dilemma by introducing a parameter $\\epsilon\\in[0,1]$, that will decrease with the number of iterations, and will be used to decide, with probability $\\epsilon$, if the agent will take a random action or will choose the action prescribed by $Q$.\n",
        "\n",
        "- We will also introduce a **replay buffer**. This will store a \"large\" number of transitions $(s, a, r, s')$ that will be used to construct training data batches to update the parameters of $Q$ using gradient descent.\n",
        "\n",
        "- Finally, using $Q$ itself to generate the targets for the loss will make the training very unstable. To circumvent this problem, we will introduce another DQN $\\hat Q$, called the **target network**, that is periodically synchronized with the main $Q$ network, but otherwise remains unchanged for a given number of iterations.   \n",
        "\n",
        "o $\\epsilon$ vai dexendo e quando chega a 0.5, metade das vezes explora, metade das vezes faz a melhor ação\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zfjMdN1OO5S"
      },
      "source": [
        "Upgraded DQN algorithm:  \n",
        "\n",
        "1. Initialize parameters for $Q(s,a)$ and $\\hat Q(s,a)$,$\\;\\epsilon \\leftarrow 1.0$, and empty the replay buffer.\n",
        "\n",
        "2. With probability $\\epsilon$, select a random action $a$; otherwise, $a = \\text{argmax}_a Q(s, a)$.\n",
        "\n",
        "3. Execute action $a$ in an emulator and observe the reward, $r$, and the next state, $s'$.\n",
        "\n",
        "4. Store the transition $(s, a, r, s')$ in the replay buffer.\n",
        "\n",
        "5. Sample a random mini-batch of transitions from the replay buffer.\n",
        "\n",
        "6. For every transition in the buffer, calculate the target:\n",
        "\n",
        "$$\n",
        "y =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " r  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " r + \\gamma \\max_{a'} \\hat Q(s' ,a') \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "7. Calculate the loss: ${\\cal L}=(Q(s,a)-y)^2$.\n",
        "\n",
        "8. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "9. Repeat from step 2 until convergence (in the sense described before) is achieved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7UB14l5VB7d"
      },
      "source": [
        "We will now present the Lapan's implementation of this algorithm, as exposed in chapter 6 of his book; see also\n",
        "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition.\n",
        "\n",
        "Note that Lapan uses PyTorch to implement the DQNs. It is a quite instructive exercise to translate the code to tensorflow/keras.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpamMumdWN-r"
      },
      "source": [
        "# **Wrappers:** For efficiency and conceptual reasons, we need to preprocess the environment. For instance:\n",
        "\n",
        "- We can reduce the game frames by considering only a monochromatic version of them and a lower resolution. This is \"hidden\" in the *atari_wrappers.AtariWrapper* class. After applying this, our images will have a shape of $(84,84,1)$; recall that originally they had a shape equal to $(210,160,3)$. Note that this class takes care of a lot of more relevant preprocessing details (see Chapter 6 of Lapan's book for more details).   \n",
        "\n",
        "- This isn't the way PyTorch is designed to receive information; it expects the form (channels, height, width). The ImageToPyTorch wrapper, presented below, takes care of this.\n",
        "\n",
        "- The agent won't be able to learn how to play Pong if we only provide still images of the game. To learn how to play Pong, we need to learn about dynamics. So we need to pack a given number (n_steps) of consecutive images into a \"small video\" with n_steps frames. This is taken care of by the BufferWrapper (see below).\n",
        "\n",
        "Here is the code for the wrappers.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JlM3EYsxZFQg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gym dá no formato (H, W, C) que é o que o tensorflow espera\n",
        "# class ImageToPyTorch(gym.ObservationWrapper):\n",
        "#     def __init__(self, env):\n",
        "#         super(ImageToPyTorch, self).__init__(env)\n",
        "#         obs = self.observation_space\n",
        "#         assert isinstance(obs, gym.spaces.Box)\n",
        "#         assert len(obs.shape) == 3\n",
        "#         new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "#         self.observation_space = gym.spaces.Box(\n",
        "#             low=obs.low.min(), high=obs.high.max(),\n",
        "#             shape=new_shape, dtype=obs.dtype)\n",
        "#\n",
        "#    def observation(self, observation):\n",
        "#        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "import typing as tt\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper): \n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            #obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            # antes os channels tavam no inicio agora estao no fim, agora temos (H, W, C * n_steps) não (C * n_steps, H, W)\n",
        "            obs.low.repeat(n_steps, axis=-1), obs.high.repeat(n_steps, axis=-1),\n",
        "            dtype=obs.dtype)\n",
        "\n",
        "        # old_shape = obs.shape\n",
        "        # new_shape = (old_shape[0], old_shape[1], old_shape[2] * n_steps)\n",
        "        # new_obs = gym.spaces.Box(\n",
        "        #     low=np.repeat(obs.low, n_steps, axis=-1),\n",
        "        #     high=np.repeat(obs.high, n_steps, axis=-1),\n",
        "        #     shape=new_shape,\n",
        "        #     dtype=obs.dtype\n",
        "        # )\n",
        "\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen-1): # preencher o buffer com frames vazias\n",
        "            self.buffer.append(self.env.observation_space.low)\n",
        "        obs, extra = self.env.reset() # reset gym env\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(list(self.buffer), axis=-1) # concat along channel (last in the list)\n",
        "        #return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, **kwargs):\n",
        "    env = gym.make(env_name, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    #env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=4)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXLwfJHZO-X"
      },
      "source": [
        "For the DQN model, we will use a typical convolution network, with a conv base followed by a dense network. Importantly, instead of modeling $Q$ as a function of the pair $(s,a)$ that outputs the corresponding value, i.e.,\n",
        "\n",
        "$$Q: {\\cal S}\\times {\\cal A} \\rightarrow \\mathbb{R}\\;,$$\n",
        "\n",
        "where, for our environment, we have\n",
        "\n",
        "$${\\cal S}=\\mathbb{R}^{84\\times 84\\times 4}$$\n",
        "\n",
        "and\n",
        "\n",
        "$${\\cal A} =\\{0,1,2,3,4,5\\} \\subset \\mathbb{R}\\;,$$\n",
        "\n",
        "we will use a dual representation   \n",
        "\n",
        "$$Q: {\\cal S} \\rightarrow \\mathbb{R}^6\\;,$$\n",
        "\n",
        "that given the state, outputs the value for each possible action.\n",
        "\n",
        "The PyTorch code should be, by now, self-explanatory. Notice nonetheless that it requires a bit more work, than the one needed in Keras to set dimensions of the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class DQN(tf.keras.Model):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "\n",
        "        print(f\"DQN input shape: {input_shape}\")\n",
        "\n",
        "        self.rescale = layers.Rescaling(1./255) # normalização aqui\n",
        "        self.conv1 = layers.Conv2D(filters=32, kernel_size=8, strides=4, activation=\"relu\", input_shape=input_shape)\n",
        "        self.conv2 = layers.Conv2D(filters=64, kernel_size=4, strides=2, activation=\"relu\")\n",
        "        self.conv3 = layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=\"relu\")\n",
        "        self.flatten  = layers.Flatten()\n",
        "\n",
        "        # fully connected\n",
        "        self.fc = layers.Dense(units=512, activation='relu')\n",
        "        self.out = layers.Dense(units=n_actions, activation = None)\n",
        "\n",
        "\n",
        "    def call(self, inputs): # requiered for subclasses of tf.keras.Model\n",
        "        x = self.rescale(inputs)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        q = self.out(x)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "J6GBp08adVU0"
      },
      "outputs": [],
      "source": [
        "# # bom exercicio é traduzir este codigo de torch para keras com tensorflow\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class DQN(nn.Module):\n",
        "#     def __init__(self, input_shape, n_actions):\n",
        "#         super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "#         #__super__().__init\n",
        "\n",
        "#         self.conv = nn.Sequential(\n",
        "#             nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), # convolução\n",
        "#             nn.ReLU(), # função de ativação relu\n",
        "#             nn.Conv2d(32, 64, kernel_size=4, stride=2), # 32 é igual ao 32 a cima, o keras faz isso automatico\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Flatten(),\n",
        "#         )\n",
        "#         size = self.conv(torch.zeros(1, *input_shape)).size()[-1] # ta a aplicar o vetor de convolução a um vetor de zeros oara ver o tamanho dele (.size = .shape)\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(size, 512), # rede densa\n",
        "#             nn.ReLU(), # relu\n",
        "#             nn.Linear(512, n_actions) # mais uma densa\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x: torch.ByteTensor):\n",
        "#         # scale on GPU\n",
        "#         xx = x / 255.0\n",
        "#         return self.fc(self.conv(xx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFQhRa3ddnF"
      },
      "source": [
        "Next we define several parameters and variables and construct the class fro replay buffer.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aG1OXGH_dvxj"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0 # começamos com probabilidade 1 de fazer algo ao calhas\n",
        "EPSILON_FINAL = 0.01 # acabamos com probabilidade 0.01 de fazer algo ao calhas\n",
        "\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    tf.Tensor,           # current state (batch, H, W, C)\n",
        "    tf.Tensor,           # actions\n",
        "    tf.Tensor,               # rewards\n",
        "    tf.Tensor,           # done || trunc\n",
        "    tf.Tensor           # next state\n",
        "]\n",
        "# BatchTensors = Tuple[\n",
        "#     np.ndarray,   # current state batch, shape (B, H, W, C)\n",
        "#     np.ndarray,   # actions batch, shape (B,)\n",
        "#     np.ndarray,   # rewards batch, shape (B,)\n",
        "#     np.ndarray,   # done flags batch, shape (B,)\n",
        "#     np.ndarray    # next state batch, shape (B, H, W, C)\n",
        "# ]\n",
        "\n",
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MNCg8vueHfi"
      },
      "source": [
        "We now create the agent class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "zknt1frZeSeI",
        "outputId": "255354e7-1eca-4e81-fbb4-734308e83de9"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def play_step(self, net: DQN,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon: # com probabilidade epsilon joga ao calhas\n",
        "            action = env.action_space.sample()\n",
        "        else: # caso contrario usa a informação do Q\n",
        "\n",
        "            #state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v = tf.convert_to_tensor(self.state, dtype=tf.float32)\n",
        "\n",
        "            #state_v.unsqueeze_(0) # gera uma dimensão, erro comum\n",
        "            state_v = tf.expand_dims(state_v, axis=0)\n",
        "\n",
        "            #q_vals_v = net(state_v) # gera os Qs\n",
        "            q_values = net(state_v)  \n",
        "\n",
        "            #_, act_v = torch.max(q_vals_v, dim=1) # queremos os maiores na dimensão 1\n",
        "            act_v = tf.argmax(q_values, axis=1)\n",
        "\n",
        "            #action = int(act_v.item()) # faz a ação que tem o melhor Q\n",
        "            action = int(act_v[0])\n",
        "            # action = int(act_idx.numpy()[0])\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action) # joga essa ação\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience( #\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        # informação esta guardada na forma (0,a,r,done,s'). e o buffer é uma lista dessas coisas\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6pMmUrjeTzi"
      },
      "source": [
        "We will also need to transform our samples taken from the buffer into tensors that can be fed to our networks. This is achieved by this simple function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JSLXGUYWeWYH"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "#def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "def batch_to_tensors(batch: List[Experience]) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    \n",
        "    #states_t = torch.as_tensor(np.asarray(states))\n",
        "    states_t = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "\n",
        "    #actions_t = torch.LongTensor(actions)\n",
        "    actions_t = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "\n",
        "    #rewards_t = torch.FloatTensor(rewards)\n",
        "    rewards_t = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "\n",
        "    #dones_t = torch.BoolTensor(dones)\n",
        "    dones_t = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
        "    \n",
        "    #new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    new_states_t = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
        "\n",
        "    #return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "    #       dones_t.to(device),  new_states_t.to(device)\n",
        "    return states_t, actions_t, rewards_t, dones_t,  new_states_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spsqJAktfDU9"
      },
      "source": [
        "A not-so-easy piece of code, even being quite small, is the one to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN): # -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "    q_values = net(states_t)\n",
        "\n",
        "    #indices = tf.stack([tf.range(BATCH_SIZE), actions_t], axis=1)\n",
        "    # q_values has shape (B, n_actions)\n",
        "    batch_range = tf.range(tf.shape(q_values)[0], dtype=actions_t.dtype)   # shape (B,)\n",
        "    indices     = tf.stack([batch_range, actions_t], axis=1)               # shape (B,2)\n",
        "\n",
        "    #state_action_values = tf.expand_dims(q_values, indices)\n",
        "    state_action_values = tf.gather_nd(q_values, indices)\n",
        "\n",
        "    next_q = tf.reduce_max(tgt_net(new_states_t), axis=1)\n",
        "    next_q = next_q * tf.cast(tf.logical_not(dones_t), tf.float32)\n",
        "\n",
        "    expected_state_action_values = next_q * GAMMA + rewards_t\n",
        "\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    loss = loss_fn(expected_state_action_values, state_action_values)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TwIK6i2ifNMj"
      },
      "outputs": [],
      "source": [
        "# def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "#               device: torch.device) -> torch.Tensor:\n",
        "#     states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "#     state_action_values = net(states_t).gather(\n",
        "#         1, actions_t.unsqueeze(-1)\n",
        "#     ).squeeze(-1) # squeeze retira uma dimensão de [[...]] para [...]\n",
        "#     with torch.no_grad():\n",
        "#         next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "#         next_state_values[dones_t] = 0.0\n",
        "#         next_state_values = next_state_values.detach() # não calcular gradientes\n",
        "\n",
        "#     expected_state_action_values = next_state_values * GAMMA + rewards_t\n",
        "#     return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84nZVAyFgCw6"
      },
      "source": [
        "Finally, we have the code for the training loop. Note that this training procedure is quite expensive it will be unfeaseble without the access to a GPU.  \n",
        "\n",
        "Ver tensorboard com `tensorboard --logdir=logs  --port=6006`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment observation space: (84, 84, 4)\n",
            "DQN input shape: (84, 84, 4)\n",
            "DQN input shape: (84, 84, 4)\n",
            "Dummy input shape: (1, 84, 84, 4)\n",
            "Network architecture:\n",
            "762: done 1 games, reward -21.000, eps 0.99, speed 1208.61 f/s\n",
            "1672: done 2 games, reward -21.000, eps 0.99, speed 1065.31 f/s\n",
            "2602: done 3 games, reward -21.000, eps 0.98, speed 1039.52 f/s\n",
            "3637: done 4 games, reward -20.500, eps 0.98, speed 1006.89 f/s\n",
            "Best reward updated -21.000 -> -20.500\n",
            "4548: done 5 games, reward -20.600, eps 0.97, speed 899.86 f/s\n",
            "5452: done 6 games, reward -20.667, eps 0.96, speed 890.05 f/s\n",
            "6479: done 7 games, reward -20.714, eps 0.96, speed 863.31 f/s\n",
            "7394: done 8 games, reward -20.625, eps 0.95, speed 805.59 f/s\n",
            "8448: done 9 games, reward -20.333, eps 0.94, speed 808.47 f/s\n",
            "Best reward updated -20.500 -> -20.333\n",
            "9210: done 10 games, reward -20.400, eps 0.94, speed 700.50 f/s\n",
            "training in frame 10000, Loss = 0.0016\n",
            "training in frame 10010, Loss = 0.0012\n",
            "training in frame 10020, Loss = 0.0616\n",
            "training in frame 10030, Loss = 0.0013\n",
            "training in frame 10040, Loss = 0.0006\n",
            "training in frame 10050, Loss = 0.0914\n",
            "training in frame 10060, Loss = 0.0007\n",
            "training in frame 10070, Loss = 0.0003\n",
            "training in frame 10080, Loss = 0.0006\n",
            "training in frame 10090, Loss = 0.0008\n",
            "training in frame 10100, Loss = 0.0008\n",
            "training in frame 10110, Loss = 0.0623\n",
            "training in frame 10120, Loss = 0.0006\n",
            "training in frame 10130, Loss = 0.0004\n",
            "training in frame 10140, Loss = 0.0005\n",
            "training in frame 10150, Loss = 0.0004\n",
            "10152: done 11 games, reward -20.455, eps 0.93, speed 7.17 f/s\n",
            "training in frame 10160, Loss = 0.0008\n",
            "training in frame 10170, Loss = 0.0004\n",
            "training in frame 10180, Loss = 0.0006\n",
            "training in frame 10190, Loss = 0.0612\n",
            "training in frame 10200, Loss = 0.0009\n",
            "training in frame 10210, Loss = 0.0604\n",
            "training in frame 10220, Loss = 0.0006\n",
            "training in frame 10230, Loss = 0.0003\n",
            "training in frame 10240, Loss = 0.0602\n",
            "training in frame 10250, Loss = 0.0007\n",
            "training in frame 10260, Loss = 0.0006\n",
            "training in frame 10270, Loss = 0.0299\n",
            "training in frame 10280, Loss = 0.0006\n",
            "training in frame 10290, Loss = 0.0007\n",
            "training in frame 10300, Loss = 0.0316\n",
            "training in frame 10310, Loss = 0.0303\n",
            "training in frame 10320, Loss = 0.0299\n",
            "training in frame 10330, Loss = 0.0892\n",
            "training in frame 10340, Loss = 0.0010\n",
            "training in frame 10350, Loss = 0.0295\n",
            "training in frame 10360, Loss = 0.0631\n",
            "training in frame 10370, Loss = 0.0606\n",
            "training in frame 10380, Loss = 0.0301\n",
            "training in frame 10390, Loss = 0.0602\n",
            "training in frame 10400, Loss = 0.0309\n",
            "training in frame 10410, Loss = 0.0005\n",
            "training in frame 10420, Loss = 0.0006\n",
            "training in frame 10430, Loss = 0.0006\n",
            "training in frame 10440, Loss = 0.0004\n",
            "training in frame 10450, Loss = 0.0593\n",
            "training in frame 10460, Loss = 0.0301\n",
            "training in frame 10470, Loss = 0.0596\n",
            "training in frame 10480, Loss = 0.0008\n",
            "training in frame 10490, Loss = 0.0595\n",
            "training in frame 10500, Loss = 0.1197\n",
            "training in frame 10510, Loss = 0.0300\n",
            "training in frame 10520, Loss = 0.0007\n",
            "training in frame 10530, Loss = 0.0606\n",
            "training in frame 10540, Loss = 0.0006\n",
            "training in frame 10550, Loss = 0.0308\n",
            "training in frame 10560, Loss = 0.0300\n",
            "training in frame 10570, Loss = 0.0598\n",
            "training in frame 10580, Loss = 0.0298\n",
            "training in frame 10590, Loss = 0.0009\n",
            "training in frame 10600, Loss = 0.0293\n",
            "training in frame 10610, Loss = 0.0008\n",
            "training in frame 10620, Loss = 0.0009\n",
            "training in frame 10630, Loss = 0.0005\n",
            "training in frame 10640, Loss = 0.0004\n",
            "training in frame 10650, Loss = 0.0007\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import argparse\n",
        "import collections\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "                    help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "#env = wrappers.make_env(args.env)\n",
        "env = make_env(args.env)\n",
        "print(f\"Environment observation space: {env.observation_space.shape}\")\n",
        "net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "# Actually initialize both models by calling them with dummy input\n",
        "dummy_input = tf.zeros((1,) + env.observation_space.shape, dtype=tf.float32)\n",
        "print(f\"Dummy input shape: {dummy_input.shape}\")\n",
        "net(dummy_input)  # This creates the weights\n",
        "tgt_net(dummy_input)  # This creates the weights\n",
        "\n",
        "\n",
        "log_dir = f\"logs/{args.env}_{int(time.time())}\"\n",
        "writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "#writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "print(\"Network architecture:\")\n",
        "#net.build((None,) + env.observation_space.shape)\n",
        "#net.summary()\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "              f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "\n",
        "        # TensorBoard logging for TensorFlow\n",
        "        with writer.as_default():\n",
        "            tf.summary.scalar(\"epsilon\", epsilon, step=frame_idx)\n",
        "            tf.summary.scalar(\"speed\", speed, step=frame_idx)\n",
        "            tf.summary.scalar(\"reward_100\", m_reward, step=frame_idx)\n",
        "            tf.summary.scalar(\"reward\", reward, step=frame_idx)\n",
        "            writer.flush()\n",
        "        #writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        #writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        #writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        #writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "\n",
        "        if best_m_reward is None or best_m_reward < m_reward:\n",
        "            #torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "            net.save_weights(args.env + \"-best_%.0f.dat\" % m_reward + \".weights.h5\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "        if m_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    # copy weights from net to tgt_net\n",
        "    #if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "    #    tgt_net.load_state_dict(net.state_dict())\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.set_weights(net.get_weights())\n",
        "\n",
        "    # optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "\n",
        "\n",
        "    #loss_t = calc_loss(batch, net, tgt_net)\n",
        "    #loss_t.backward()\n",
        "    #optimizer.step()\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_t = calc_loss(batch, net, tgt_net)\n",
        "    grads = tape.gradient(loss_t, net.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
        "\n",
        "    if frame_idx % 10 == 0:\n",
        "        print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-9QeKX_f13p"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import argparse\n",
        "# import collections\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "# parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "#                     help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "# args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "# #env = wrappers.make_env(args.env)\n",
        "# env = make_env(args.env)\n",
        "# net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "# print(net)\n",
        "\n",
        "# buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "# agent = Agent(env, buffer)\n",
        "# epsilon = EPSILON_START\n",
        "\n",
        "# optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "# total_rewards = []\n",
        "# frame_idx = 0\n",
        "# ts_frame = 0\n",
        "# ts = time.time()\n",
        "# best_m_reward = None\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     frame_idx += 1\n",
        "#     epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "#     reward = agent.play_step(net, device, epsilon)\n",
        "#     if reward is not None:\n",
        "#         total_rewards.append(reward)\n",
        "#         speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "#         ts_frame = frame_idx\n",
        "#         ts = time.time()\n",
        "#         m_reward = np.mean(total_rewards[-100:])\n",
        "#         print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "#               f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "#         writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "#         writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "#         writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "#         writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "#         if best_m_reward is None or best_m_reward < m_reward:\n",
        "#             torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "#             if best_m_reward is not None:\n",
        "#                 print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "#             best_m_reward = m_reward\n",
        "#         if m_reward > MEAN_REWARD_BOUND:\n",
        "#             print(\"Solved in %d frames!\" % frame_idx)\n",
        "#             break\n",
        "#     if len(buffer) < REPLAY_START_SIZE:\n",
        "#         continue\n",
        "#     if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "#         tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "#     optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "#     batch = buffer.sample(BATCH_SIZE)\n",
        "#     loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "#     loss_t.backward()\n",
        "#     optimizer.step()\n",
        "#     if frame_idx % 10 == 0:\n",
        "#         print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "# writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgrTii2EhdQZ"
      },
      "source": [
        "The evolution of our agent's ability to play pong is recorded in the following collection of videos\n",
        "https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
