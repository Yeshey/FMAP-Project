{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_yxCfjvnS_I"
      },
      "source": [
        "# Deep Q Learning.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9p2ibhhp-ta"
      },
      "source": [
        "If the number of states becomes too large, tabular learning becomes unfeasible. A way to circumvent this problem is to model $Q$ as a deep neural network that receives as inputs the state and the action, and outputs the corresponding $Q$ value. The parameters of this network can then be learned via gradient descent by considering a loss that measures how far $Q$ is from satisfying Bellman's equation.\n",
        "\n",
        "This is the basic conceptual idea behind the major breakthrough made in 2013 by a team from DeepMind https://arxiv.org/abs/1312.5602, which allowed them to train a DQN (Deep Q Network) to play several Atari games; in fact, the networke learned \"by itself\" starting from the pixel data of the frames of the computer games. This remarkable breakthrough was the starting point to a spectacular series of achievements that included mastering high-complexity strategy games such as chess and Go, and most importantly, the computation of the 3D structure of proteins https://www.nobelprize.org/prizes/chemistry/2024/summary/.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AappnB_LtXdi"
      },
      "source": [
        "A naive version of the DQN algorithm allude to above, can be summarizes as follows:\n",
        "\n",
        "1. Initialize $Q(s, a)$ using a standard deep learning initialization.\n",
        "\n",
        "2. By interacting with the environment, obtain the tuple $(s, a, r, s')$.\n",
        "\n",
        "3. Compute the loss:\n",
        "\n",
        "$${\\cal L} =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " (Q(s, a) - r)^2  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " (Q(s, a) - (r + \\gamma \\max_{a'} Q(s' ,a'))^2 \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "4. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "5. Repeat from step 2 until convergence (in the sense described for tabular-$Q$ learning) is achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCs21Aj2b50h"
      },
      "source": [
        "We'll now focus on a single Atari game, the iconic Pong https://ale.farama.org/environments/pong/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PhOeajsRcnXZ",
        "outputId": "7d16ec35-c3b7-496b-f702-ac621af9973c"
      },
      "outputs": [],
      "source": [
        "# !pip install 'stable_baselines3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "MZeLCmSQb5IR",
        "outputId": "9f89fbec-7b4a-4b1c-9515-b08739f1c4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(210, 160, 3)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]]], dtype=uint8)"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "import ale_py\n",
        "\n",
        "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "\n",
        "obs,_ = env.reset()\n",
        "\n",
        "print(obs.shape)\n",
        "\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSoytnibWas"
      },
      "source": [
        "Unfortunately, the previous version of DQN doesn't work very well. So we need to do perform some upgrades.\n",
        "\n",
        "- On one hand, we need to explore the environment (using random actions); on the other, we want to use the knowledge gained by the $Q$-function; this is the famous \"exploration versus exploitation dilemma\".  We will resolve this dilemma by introducing a parameter $\\epsilon\\in[0,1]$, that will decrease with the number of iterations, and will be used to decide, with probability $\\epsilon$, if the agent will take a random action or will choose the action prescribed by $Q$.\n",
        "\n",
        "- We will also introduce a **replay buffer**. This will store a \"large\" number of transitions $(s, a, r, s')$ that will be used to construct training data batches to update the parameters of $Q$ using gradient descent.\n",
        "\n",
        "- Finally, using $Q$ itself to generate the targets for the loss will make the training very unstable. To circumvent this problem, we will introduce another DQN $\\hat Q$, called the **target network**, that is periodically synchronized with the main $Q$ network, but otherwise remains unchanged for a given number of iterations.   \n",
        "\n",
        "o $\\epsilon$ vai dexendo e quando chega a 0.5, metade das vezes explora, metade das vezes faz a melhor ação\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zfjMdN1OO5S"
      },
      "source": [
        "Upgraded DQN algorithm:  \n",
        "\n",
        "1. Initialize parameters for $Q(s,a)$ and $\\hat Q(s,a)$,$\\;\\epsilon \\leftarrow 1.0$, and empty the replay buffer.\n",
        "\n",
        "2. With probability $\\epsilon$, select a random action $a$; otherwise, $a = \\text{argmax}_a Q(s, a)$.\n",
        "\n",
        "3. Execute action $a$ in an emulator and observe the reward, $r$, and the next state, $s'$.\n",
        "\n",
        "4. Store the transition $(s, a, r, s')$ in the replay buffer.\n",
        "\n",
        "5. Sample a random mini-batch of transitions from the replay buffer.\n",
        "\n",
        "6. For every transition in the buffer, calculate the target:\n",
        "\n",
        "$$\n",
        "y =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " r  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " r + \\gamma \\max_{a'} \\hat Q(s' ,a') \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "7. Calculate the loss: ${\\cal L}=(Q(s,a)-y)^2$.\n",
        "\n",
        "8. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "9. Repeat from step 2 until convergence (in the sense described before) is achieved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7UB14l5VB7d"
      },
      "source": [
        "We will now present the Lapan's implementation of this algorithm, as exposed in chapter 6 of his book; see also\n",
        "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition.\n",
        "\n",
        "Note that Lapan uses PyTorch to implement the DQNs. It is a quite instructive exercise to translate the code to tensorflow/keras.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpamMumdWN-r"
      },
      "source": [
        "# **Wrappers:** For efficiency and conceptual reasons, we need to preprocess the environment. For instance:\n",
        "\n",
        "- We can reduce the game frames by considering only a monochromatic version of them and a lower resolution. This is \"hidden\" in the *atari_wrappers.AtariWrapper* class. After applying this, our images will have a shape of $(84,84,1)$; recall that originally they had a shape equal to $(210,160,3)$. Note that this class takes care of a lot of more relevant preprocessing details (see Chapter 6 of Lapan's book for more details).   \n",
        "\n",
        "- This isn't the way PyTorch is designed to receive information; it expects the form (channels, height, width). The ImageToPyTorch wrapper, presented below, takes care of this.\n",
        "\n",
        "- The agent won't be able to learn how to play Pong if we only provide still images of the game. To learn how to play Pong, we need to learn about dynamics. So we need to pack a given number (n_steps) of consecutive images into a \"small video\" with n_steps frames. This is taken care of by the BufferWrapper (see below).\n",
        "\n",
        "Here is the code for the wrappers.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "JlM3EYsxZFQg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gym dá no formato (H, W, C) que é o que o tensorflow espera\n",
        "# class ImageToPyTorch(gym.ObservationWrapper):\n",
        "#     def __init__(self, env):\n",
        "#         super(ImageToPyTorch, self).__init__(env)\n",
        "#         obs = self.observation_space\n",
        "#         assert isinstance(obs, gym.spaces.Box)\n",
        "#         assert len(obs.shape) == 3\n",
        "#         new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "#         self.observation_space = gym.spaces.Box(\n",
        "#             low=obs.low.min(), high=obs.high.max(),\n",
        "#             shape=new_shape, dtype=obs.dtype)\n",
        "#\n",
        "#    def observation(self, observation):\n",
        "#        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "import typing as tt\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper): \n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            #obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            # antes os channels tavam no inicio agora estao no fim, agora temos (H, W, C * n_steps) não (C * n_steps, H, W)\n",
        "            obs.low.repeat(n_steps, axis=-1), obs.high.repeat(n_steps, axis=-1),\n",
        "            dtype=obs.dtype)\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen-1): # preencher o buffer com frames vazias\n",
        "            self.buffer.append(self.env.observation_space.low)\n",
        "        obs, extra = self.env.reset() # reset gym env\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, **kwargs):\n",
        "    env = gym.make(env_name, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    #env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=4)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXLwfJHZO-X"
      },
      "source": [
        "For the DQN model, we will use a typical convolution network, with a conv base followed by a dense network. Importantly, instead of modeling $Q$ as a function of the pair $(s,a)$ that outputs the corresponding value, i.e.,\n",
        "\n",
        "$$Q: {\\cal S}\\times {\\cal A} \\rightarrow \\mathbb{R}\\;,$$\n",
        "\n",
        "where, for our environment, we have\n",
        "\n",
        "$${\\cal S}=\\mathbb{R}^{84\\times 84\\times 4}$$\n",
        "\n",
        "and\n",
        "\n",
        "$${\\cal A} =\\{0,1,2,3,4,5\\} \\subset \\mathbb{R}\\;,$$\n",
        "\n",
        "we will use a dual representation   \n",
        "\n",
        "$$Q: {\\cal S} \\rightarrow \\mathbb{R}^6\\;,$$\n",
        "\n",
        "that given the state, outputs the value for each possible action.\n",
        "\n",
        "The PyTorch code should be, by now, self-explanatory. Notice nonetheless that it requires a bit more work, than the one needed in Keras to set dimensions of the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class DQN(tf.keras.Model):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "\n",
        "        print(input_shape)\n",
        "\n",
        "        self.rescale = layers.Rescaling(1./255) # normalização aqui\n",
        "        self.conv1 = layers.Conv2D(filters=32, kernel_size=8, strides=4, activation=\"relu\", input_shape=input_shape)\n",
        "        self.conv2 = layers.Conv2D(filters=64, kernel_size=4, strides=2, activation=\"relu\")\n",
        "        self.conv3 = layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=\"relu\")\n",
        "        self.flatten  = layers.Flatten()\n",
        "\n",
        "        # fully connected\n",
        "        self.fc = layers.Dense(units=512, activation='relu')\n",
        "        self.out = layers.Dense(units=n_actions, activation = None)\n",
        "\n",
        "\n",
        "    def call(self, inputs): # requiered for subclasses of tf.keras.Model\n",
        "        x = self.rescale(inputs)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        q = self.out(x)\n",
        "        return q\n",
        "\n",
        "\n",
        "class DQN(tf.keras.Model):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "\n",
        "        print(input_shape)\n",
        "\n",
        "        self.rescale = layers.Rescaling(1./255) # normalização aqui\n",
        "        self.conv1 = layers.Conv2D(filters=32, kernel_size=8, strides=4, activation=\"relu\", input_shape=input_shape)\n",
        "        self.conv2 = layers.Conv2D(filters=64, kernel_size=4, strides=2, activation=\"relu\")\n",
        "        self.conv3 = layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=\"relu\")\n",
        "        self.flatten  = layers.Flatten()\n",
        "\n",
        "        # fully connected\n",
        "        self.fc = layers.Dense(units=512, activation='relu')\n",
        "        self.out = layers.Dense(units=n_actions, activation = None)\n",
        "\n",
        "\n",
        "    def call(self, inputs): # requiered for subclasses of tf.keras.Model\n",
        "        x = self.rescale(inputs)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        q = self.out(x)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "J6GBp08adVU0"
      },
      "outputs": [],
      "source": [
        "# # bom exercicio é traduzir este codigo de torch para keras com tensorflow\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class DQN(nn.Module):\n",
        "#     def __init__(self, input_shape, n_actions):\n",
        "#         super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "#         #__super__().__init\n",
        "\n",
        "#         self.conv = nn.Sequential(\n",
        "#             nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), # convolução\n",
        "#             nn.ReLU(), # função de ativação relu\n",
        "#             nn.Conv2d(32, 64, kernel_size=4, stride=2), # 32 é igual ao 32 a cima, o keras faz isso automatico\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Flatten(),\n",
        "#         )\n",
        "#         size = self.conv(torch.zeros(1, *input_shape)).size()[-1] # ta a aplicar o vetor de convolução a um vetor de zeros oara ver o tamanho dele (.size = .shape)\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(size, 512), # rede densa\n",
        "#             nn.ReLU(), # relu\n",
        "#             nn.Linear(512, n_actions) # mais uma densa\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x: torch.ByteTensor):\n",
        "#         # scale on GPU\n",
        "#         xx = x / 255.0\n",
        "#         return self.fc(self.conv(xx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFQhRa3ddnF"
      },
      "source": [
        "Next we define several parameters and variables and construct the class fro replay buffer.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "aG1OXGH_dvxj"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0 # começamos com probabilidade 1 de fazer algo ao calhas\n",
        "EPSILON_FINAL = 0.01 # acabamos com probabilidade 0.01 de fazer algo ao calhas\n",
        "\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    tf.Tensor,           # current state (batch, H, W, C)\n",
        "    tf.Tensor,           # actions\n",
        "    tf.Tensor,               # rewards\n",
        "    tf.Tensor,           # done || trunc\n",
        "    tf.Tensor           # next state\n",
        "]\n",
        "# BatchTensors = Tuple[\n",
        "#     np.ndarray,   # current state batch, shape (B, H, W, C)\n",
        "#     np.ndarray,   # actions batch, shape (B,)\n",
        "#     np.ndarray,   # rewards batch, shape (B,)\n",
        "#     np.ndarray,   # done flags batch, shape (B,)\n",
        "#     np.ndarray    # next state batch, shape (B, H, W, C)\n",
        "# ]\n",
        "\n",
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MNCg8vueHfi"
      },
      "source": [
        "We now create the agent class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "zknt1frZeSeI",
        "outputId": "255354e7-1eca-4e81-fbb4-734308e83de9"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def play_step(self, net: DQN,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon: # com probabilidade epsilon joga ao calhas\n",
        "            action = env.action_space.sample()\n",
        "        else: # caso contrario usa a informação do Q\n",
        "\n",
        "            #state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v = tf.convert_to_tensor(self.state, dtype=tf.float32)\n",
        "\n",
        "            #state_v.unsqueeze_(0) # gera uma dimensão, erro comum\n",
        "            state_v = tf.expand_dims(state_v, axis=0)\n",
        "\n",
        "            #q_vals_v = net(state_v) # gera os Qs\n",
        "            q_values = net(state_v)  \n",
        "\n",
        "            #_, act_v = torch.max(q_vals_v, dim=1) # queremos os maiores na dimensão 1\n",
        "            act_v = tf.argmax(q_values, axis=1)\n",
        "\n",
        "            #action = int(act_v.item()) # faz a ação que tem o melhor Q\n",
        "            action = int(act_v[0])\n",
        "            # action = int(act_idx.numpy()[0])\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action) # joga essa ação\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience( #\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        # informação esta guardada na forma (0,a,r,done,s'). e o buffer é uma lista dessas coisas\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6pMmUrjeTzi"
      },
      "source": [
        "We will also need to transform our samples taken from the buffer into tensors that can be fed to our networks. This is achieved by this simple function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "JSLXGUYWeWYH"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "#def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "def batch_to_tensors(batch: List[Experience]) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    \n",
        "    #states_t = torch.as_tensor(np.asarray(states))\n",
        "    states_t = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "\n",
        "    #actions_t = torch.LongTensor(actions)\n",
        "    actions_t = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "\n",
        "    #rewards_t = torch.FloatTensor(rewards)\n",
        "    rewards_t = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "\n",
        "    #dones_t = torch.BoolTensor(dones)\n",
        "    dones_t = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
        "    \n",
        "    #new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    new_states_t = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
        "\n",
        "    #return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "    #       dones_t.to(device),  new_states_t.to(device)\n",
        "    return states_t, actions_t, rewards_t, dones_t,  new_states_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spsqJAktfDU9"
      },
      "source": [
        "A not-so-easy piece of code, even being quite small, is the one to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN): # -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "    q_values = net(states_t)\n",
        "\n",
        "    indices = tf.stack([tf.range(batch_size), actions_t], axis=1)\n",
        "    state_action_values = tf.expand_dims(q_values, indices)\n",
        "\n",
        "    next_q = tf.reduce_max(tgt_net(new_states_t), axis=1)\n",
        "    next_q = next_q * tf.cast(tf.logical_not(dones_t), tf.float32)\n",
        "\n",
        "    expected_state_action_values = next_q * GAMMA + rewards_t\n",
        "\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    loss = loss_fn(expected_state_action_values, state_action_values)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "TwIK6i2ifNMj"
      },
      "outputs": [],
      "source": [
        "# def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "#               device: torch.device) -> torch.Tensor:\n",
        "#     states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "#     state_action_values = net(states_t).gather(\n",
        "#         1, actions_t.unsqueeze(-1)\n",
        "#     ).squeeze(-1) # squeeze retira uma dimensão de [[...]] para [...]\n",
        "#     with torch.no_grad():\n",
        "#         next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "#         next_state_values[dones_t] = 0.0\n",
        "#         next_state_values = next_state_values.detach() # não calcular gradientes\n",
        "\n",
        "#     expected_state_action_values = next_state_values * GAMMA + rewards_t\n",
        "#     return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84nZVAyFgCw6"
      },
      "source": [
        "Finally, we have the code for the training loop. Note that this training procedure is quite expensive it will be unfeaseble without the access to a GPU.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(84, 84, 4)\n",
            "(84, 84, 4)\n",
            "<DQN name=dqn_16, built=False>\n",
            "1068: done 1 games, reward -19.000, eps 0.99, speed 1080.27 f/s\n",
            "1978: done 2 games, reward -20.000, eps 0.99, speed 887.27 f/s\n",
            "2955: done 3 games, reward -20.000, eps 0.98, speed 1021.34 f/s\n",
            "3834: done 4 games, reward -20.250, eps 0.97, speed 998.76 f/s\n",
            "4773: done 5 games, reward -20.400, eps 0.97, speed 903.57 f/s\n",
            "5990: done 6 games, reward -20.000, eps 0.96, speed 815.29 f/s\n",
            "7011: done 7 games, reward -20.000, eps 0.95, speed 936.55 f/s\n",
            "7981: done 8 games, reward -20.000, eps 0.95, speed 836.43 f/s\n",
            "8895: done 9 games, reward -20.000, eps 0.94, speed 701.63 f/s\n",
            "9759: done 10 games, reward -20.000, eps 0.93, speed 762.52 f/s\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "You called `set_weights(weights)` on layer 'dqn_17' with a weight list of length 10, but the layer was expecting 0 weights.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# copy weights from net to tgt_net\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m#if frame_idx % SYNC_TARGET_FRAMES == 0:\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m#    tgt_net.load_state_dict(net.state_dict())\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame_idx % SYNC_TARGET_FRAMES == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[43mtgt_net\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\u001b[39;00m\n\u001b[32m     80\u001b[39m batch = buffer.sample(BATCH_SIZE)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/layers/layer.py:731\u001b[39m, in \u001b[36mLayer.set_weights\u001b[39m\u001b[34m(self, weights)\u001b[39m\n\u001b[32m    729\u001b[39m layer_weights = \u001b[38;5;28mself\u001b[39m.weights\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_weights) != \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    732\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou called `set_weights(weights)` on layer \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    733\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a weight list of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but the layer \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    734\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwas expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    735\u001b[39m     )\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variable, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(layer_weights, weights):\n\u001b[32m    737\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m variable.shape != value.shape:\n",
            "\u001b[31mValueError\u001b[39m: You called `set_weights(weights)` on layer 'dqn_17' with a weight list of length 10, but the layer was expecting 0 weights."
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import argparse\n",
        "import collections\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "                    help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "#env = wrappers.make_env(args.env)\n",
        "env = make_env(args.env)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "\n",
        "#writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "              f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "\n",
        "        # TODO do tensorboard writer\n",
        "        #writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        #writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        #writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        #writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "\n",
        "        if best_m_reward is None or best_m_reward < m_reward:\n",
        "            #torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "            net.save_weights(args.env + \"-best_%.0f.dat\" % m_reward + \".weights.h5\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "        if m_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    # copy weights from net to tgt_net\n",
        "    #if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "    #    tgt_net.load_state_dict(net.state_dict())\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.set_weights(net.get_weights())\n",
        "\n",
        "    # optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "    if frame_idx % 10 == 0:\n",
        "        print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-9QeKX_f13p"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import argparse\n",
        "# import collections\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "# parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "#                     help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "# args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "# #env = wrappers.make_env(args.env)\n",
        "# env = make_env(args.env)\n",
        "# net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "# print(net)\n",
        "\n",
        "# buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "# agent = Agent(env, buffer)\n",
        "# epsilon = EPSILON_START\n",
        "\n",
        "# optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "# total_rewards = []\n",
        "# frame_idx = 0\n",
        "# ts_frame = 0\n",
        "# ts = time.time()\n",
        "# best_m_reward = None\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     frame_idx += 1\n",
        "#     epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "#     reward = agent.play_step(net, device, epsilon)\n",
        "#     if reward is not None:\n",
        "#         total_rewards.append(reward)\n",
        "#         speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "#         ts_frame = frame_idx\n",
        "#         ts = time.time()\n",
        "#         m_reward = np.mean(total_rewards[-100:])\n",
        "#         print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "#               f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "#         writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "#         writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "#         writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "#         writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "#         if best_m_reward is None or best_m_reward < m_reward:\n",
        "#             torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "#             if best_m_reward is not None:\n",
        "#                 print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "#             best_m_reward = m_reward\n",
        "#         if m_reward > MEAN_REWARD_BOUND:\n",
        "#             print(\"Solved in %d frames!\" % frame_idx)\n",
        "#             break\n",
        "#     if len(buffer) < REPLAY_START_SIZE:\n",
        "#         continue\n",
        "#     if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "#         tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "#     optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "#     batch = buffer.sample(BATCH_SIZE)\n",
        "#     loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "#     loss_t.backward()\n",
        "#     optimizer.step()\n",
        "#     if frame_idx % 10 == 0:\n",
        "#         print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "# writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgrTii2EhdQZ"
      },
      "source": [
        "The evolution of our agent's ability to play pong is recorded in the following collection of videos\n",
        "https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NdwRG-rhvzl"
      },
      "source": [
        "# Worksheet #9: Rewrite Laplan's code using TensorFlow and Keras."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
