{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_yxCfjvnS_I"
      },
      "source": [
        "# Deep Q Learning.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9p2ibhhp-ta"
      },
      "source": [
        "If the number of states becomes too large, tabular learning becomes unfeasible. A way to circumvent this problem is to model $Q$ as a deep neural network that receives as inputs the state and the action, and outputs the corresponding $Q$ value. The parameters of this network can then be learned via gradient descent by considering a loss that measures how far $Q$ is from satisfying Bellman's equation.\n",
        "\n",
        "This is the basic conceptual idea behind the major breakthrough made in 2013 by a team from DeepMind https://arxiv.org/abs/1312.5602, which allowed them to train a DQN (Deep Q Network) to play several Atari games; in fact, the networke learned \"by itself\" starting from the pixel data of the frames of the computer games. This remarkable breakthrough was the starting point to a spectacular series of achievements that included mastering high-complexity strategy games such as chess and Go, and most importantly, the computation of the 3D structure of proteins https://www.nobelprize.org/prizes/chemistry/2024/summary/.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AappnB_LtXdi"
      },
      "source": [
        "A naive version of the DQN algorithm allude to above, can be summarizes as follows:\n",
        "\n",
        "1. Initialize $Q(s, a)$ using a standard deep learning initialization.\n",
        "\n",
        "2. By interacting with the environment, obtain the tuple $(s, a, r, s')$.\n",
        "\n",
        "3. Compute the loss:\n",
        "\n",
        "$${\\cal L} =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " (Q(s, a) - r)^2  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " (Q(s, a) - (r + \\gamma \\max_{a'} Q(s' ,a'))^2 \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "4. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "5. Repeat from step 2 until convergence (in the sense described for tabular-$Q$ learning) is achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCs21Aj2b50h"
      },
      "source": [
        "We'll now focus on a single Atari game, the iconic Pong https://ale.farama.org/environments/pong/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PhOeajsRcnXZ",
        "outputId": "7d16ec35-c3b7-496b-f702-ac621af9973c"
      },
      "outputs": [],
      "source": [
        "# !pip install 'stable_baselines3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "MZeLCmSQb5IR",
        "outputId": "9f89fbec-7b4a-4b1c-9515-b08739f1c4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(210, 160, 3)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]]], dtype=uint8)"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "import ale_py\n",
        "\n",
        "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "\n",
        "obs,_ = env.reset()\n",
        "\n",
        "print(obs.shape)\n",
        "\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSoytnibWas"
      },
      "source": [
        "Unfortunately, the previous version of DQN doesn't work very well. So we need to do perform some upgrades.\n",
        "\n",
        "- On one hand, we need to explore the environment (using random actions); on the other, we want to use the knowledge gained by the $Q$-function; this is the famous \"exploration versus exploitation dilemma\".  We will resolve this dilemma by introducing a parameter $\\epsilon\\in[0,1]$, that will decrease with the number of iterations, and will be used to decide, with probability $\\epsilon$, if the agent will take a random action or will choose the action prescribed by $Q$.\n",
        "\n",
        "- We will also introduce a **replay buffer**. This will store a \"large\" number of transitions $(s, a, r, s')$ that will be used to construct training data batches to update the parameters of $Q$ using gradient descent.\n",
        "\n",
        "- Finally, using $Q$ itself to generate the targets for the loss will make the training very unstable. To circumvent this problem, we will introduce another DQN $\\hat Q$, called the **target network**, that is periodically synchronized with the main $Q$ network, but otherwise remains unchanged for a given number of iterations.   \n",
        "\n",
        "o $\\epsilon$ vai dexendo e quando chega a 0.5, metade das vezes explora, metade das vezes faz a melhor ação\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zfjMdN1OO5S"
      },
      "source": [
        "Upgraded DQN algorithm:  \n",
        "\n",
        "1. Initialize parameters for $Q(s,a)$ and $\\hat Q(s,a)$,$\\;\\epsilon \\leftarrow 1.0$, and empty the replay buffer.\n",
        "\n",
        "2. With probability $\\epsilon$, select a random action $a$; otherwise, $a = \\text{argmax}_a Q(s, a)$.\n",
        "\n",
        "3. Execute action $a$ in an emulator and observe the reward, $r$, and the next state, $s'$.\n",
        "\n",
        "4. Store the transition $(s, a, r, s')$ in the replay buffer.\n",
        "\n",
        "5. Sample a random mini-batch of transitions from the replay buffer.\n",
        "\n",
        "6. For every transition in the buffer, calculate the target:\n",
        "\n",
        "$$\n",
        "y =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " r  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " r + \\gamma \\max_{a'} \\hat Q(s' ,a') \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "7. Calculate the loss: ${\\cal L}=(Q(s,a)-y)^2$.\n",
        "\n",
        "8. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "9. Repeat from step 2 until convergence (in the sense described before) is achieved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7UB14l5VB7d"
      },
      "source": [
        "We will now present the Lapan's implementation of this algorithm, as exposed in chapter 6 of his book; see also\n",
        "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition.\n",
        "\n",
        "Note that Lapan uses PyTorch to implement the DQNs. It is a quite instructive exercise to translate the code to tensorflow/keras.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpamMumdWN-r"
      },
      "source": [
        "# **Wrappers:** For efficiency and conceptual reasons, we need to preprocess the environment. For instance:\n",
        "\n",
        "- We can reduce the game frames by considering only a monochromatic version of them and a lower resolution. This is \"hidden\" in the *atari_wrappers.AtariWrapper* class. After applying this, our images will have a shape of $(84,84,1)$; recall that originally they had a shape equal to $(210,160,3)$. Note that this class takes care of a lot of more relevant preprocessing details (see Chapter 6 of Lapan's book for more details).   \n",
        "\n",
        "- This isn't the way PyTorch is designed to receive information; it expects the form (channels, height, width). The ImageToPyTorch wrapper, presented below, takes care of this.\n",
        "\n",
        "- The agent won't be able to learn how to play Pong if we only provide still images of the game. To learn how to play Pong, we need to learn about dynamics. So we need to pack a given number (n_steps) of consecutive images into a \"small video\" with n_steps frames. This is taken care of by the BufferWrapper (see below).\n",
        "\n",
        "Here is the code for the wrappers.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "JlM3EYsxZFQg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        obs = self.observation_space\n",
        "        assert isinstance(obs, gym.spaces.Box)\n",
        "        assert len(obs.shape) == 3\n",
        "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs.low.min(), high=obs.high.max(),\n",
        "            shape=new_shape, dtype=obs.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            dtype=obs.dtype)\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen-1):\n",
        "            self.buffer.append(self.env.observation_space.low)\n",
        "        obs, extra = self.env.reset()\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, **kwargs):\n",
        "    env = gym.make(env_name, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=4)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXLwfJHZO-X"
      },
      "source": [
        "For the DQN model, we will use a typical convolution network, with a conv base followed by a dense network. Importantly, instead of modeling $Q$ as a function of the pair $(s,a)$ that outputs the corresponding value, i.e.,\n",
        "\n",
        "$$Q: {\\cal S}\\times {\\cal A} \\rightarrow \\mathbb{R}\\;,$$\n",
        "\n",
        "where, for our environment, we have\n",
        "\n",
        "$${\\cal S}=\\mathbb{R}^{84\\times 84\\times 4}$$\n",
        "\n",
        "and\n",
        "\n",
        "$${\\cal A} =\\{0,1,2,3,4,5\\} \\subset \\mathbb{R}\\;,$$\n",
        "\n",
        "we will use a dual representation   \n",
        "\n",
        "$$Q: {\\cal S} \\rightarrow \\mathbb{R}^6\\;,$$\n",
        "\n",
        "that given the state, outputs the value for each possible action.\n",
        "\n",
        "The PyTorch code should be, by now, self-explanatory. Notice nonetheless that it requires a bit more work, than the one needed in Keras to set dimensions of the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class DQN(tf.keras.Model):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "\n",
        "        print(input_shape)\n",
        "\n",
        "        self.rescale = layers.Rescaling(1./255) # normalização aqui\n",
        "        self.conv1 = layers.Conv2D(filters=32, kernel_size=8, strides=4, activation=\"relu\", input_shape=input_shape)\n",
        "        self.conv2 = layers.Conv2D(filters=64, kernel_size=4, strides=2, activation=\"relu\")\n",
        "        self.conv3 = layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=\"relu\")\n",
        "        self.flatten  = layers.Flatten()\n",
        "\n",
        "        # fully connected\n",
        "        self.fc = layers.Dense(units=512, activation='relu')\n",
        "        self.out = layers.Dense(units=n_actions, activation = None)\n",
        "\n",
        "\n",
        "    def call(self, inputs): # requiered for subclasses of tf.keras.Model\n",
        "        x = self.rescale(inputs)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        q = self.out(x)\n",
        "        return q\n",
        "\n",
        "\n",
        "class DQN(tf.keras.Model):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "\n",
        "        print(input_shape)\n",
        "\n",
        "        self.rescale = layers.Rescaling(1./255) # normalização aqui\n",
        "        self.conv1 = layers.Conv2D(filters=32, kernel_size=8, strides=4, activation=\"relu\", input_shape=input_shape)\n",
        "        self.conv2 = layers.Conv2D(filters=64, kernel_size=4, strides=2, activation=\"relu\")\n",
        "        self.conv3 = layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=\"relu\")\n",
        "        self.flatten  = layers.Flatten()\n",
        "\n",
        "        # fully connected\n",
        "        self.fc = layers.Dense(units=512, activation='relu')\n",
        "        self.out = layers.Dense(units=n_actions, activation = None)\n",
        "\n",
        "\n",
        "    def call(self, inputs): # requiered for subclasses of tf.keras.Model\n",
        "        x = self.rescale(inputs)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        q = self.out(x)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "J6GBp08adVU0"
      },
      "outputs": [],
      "source": [
        "# # bom exercicio é traduzir este codigo de torch para keras com tensorflow\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class DQN(nn.Module):\n",
        "#     def __init__(self, input_shape, n_actions):\n",
        "#         super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "#         #__super__().__init\n",
        "\n",
        "#         self.conv = nn.Sequential(\n",
        "#             nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), # convolução\n",
        "#             nn.ReLU(), # função de ativação relu\n",
        "#             nn.Conv2d(32, 64, kernel_size=4, stride=2), # 32 é igual ao 32 a cima, o keras faz isso automatico\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Flatten(),\n",
        "#         )\n",
        "#         size = self.conv(torch.zeros(1, *input_shape)).size()[-1] # ta a aplicar o vetor de convolução a um vetor de zeros oara ver o tamanho dele (.size = .shape)\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(size, 512), # rede densa\n",
        "#             nn.ReLU(), # relu\n",
        "#             nn.Linear(512, n_actions) # mais uma densa\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x: torch.ByteTensor):\n",
        "#         # scale on GPU\n",
        "#         xx = x / 255.0\n",
        "#         return self.fc(self.conv(xx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFQhRa3ddnF"
      },
      "source": [
        "Next we define several parameters and variables and construct the class fro replay buffer.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "aG1OXGH_dvxj"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0 # começamos com probabilidade 1 de fazer algo ao calhas\n",
        "EPSILON_FINAL = 0.01 # acabamos com probabilidade 0.01 de fazer algo ao calhas\n",
        "\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    tf.Tensor,           # current state (batch, H, W, C)\n",
        "    tf.Tensor,           # actions\n",
        "    tf.Tensor,               # rewards\n",
        "    tf.Tensor,           # done || trunc\n",
        "    tf.Tensor           # next state\n",
        "]\n",
        "# BatchTensors = Tuple[\n",
        "#     np.ndarray,   # current state batch, shape (B, H, W, C)\n",
        "#     np.ndarray,   # actions batch, shape (B,)\n",
        "#     np.ndarray,   # rewards batch, shape (B,)\n",
        "#     np.ndarray,   # done flags batch, shape (B,)\n",
        "#     np.ndarray    # next state batch, shape (B, H, W, C)\n",
        "# ]\n",
        "\n",
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MNCg8vueHfi"
      },
      "source": [
        "We now create the agent class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "zknt1frZeSeI",
        "outputId": "255354e7-1eca-4e81-fbb4-734308e83de9"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def play_step(self, net: DQN, device: torch.device,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon: # com probabilidade epsilon joga ao calhas\n",
        "            action = env.action_space.sample()\n",
        "        else: # caso contrario usa a informação do Q\n",
        "\n",
        "            #state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v = tf.convert_to_tensor(self.state, dtype=tf.float32)\n",
        "\n",
        "            #state_v.unsqueeze_(0) # gera uma dimensão, erro comum\n",
        "            state_v = tf.expand_dims(state_v, axis=0)\n",
        "\n",
        "            #q_vals_v = net(state_v) # gera os Qs\n",
        "            q_values = self.dqn(tf_state_batch)  \n",
        "\n",
        "            #_, act_v = torch.max(q_vals_v, dim=1) # queremos os maiores na dimensão 1\n",
        "            act_v = tf.argmax(q_values, axis=1)\n",
        "\n",
        "            #action = int(act_v.item()) # faz a ação que tem o melhor Q\n",
        "            action = int(act_v[0])\n",
        "            # action = int(act_idx.numpy()[0])\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action) # joga essa ação\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience( #\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        # informação esta guardada na forma (0,a,r,done,s'). e o buffer é uma lista dessas coisas\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6pMmUrjeTzi"
      },
      "source": [
        "We will also need to transform our samples taken from the buffer into tensors that can be fed to our networks. This is achieved by this simple function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "JSLXGUYWeWYH"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "#def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "def batch_to_tensors(batch: List[Experience]) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    \n",
        "    #states_t = torch.as_tensor(np.asarray(states))\n",
        "    states_t = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "\n",
        "    #actions_t = torch.LongTensor(actions)\n",
        "    actions_t = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "\n",
        "    #rewards_t = torch.FloatTensor(rewards)\n",
        "    rewards_t = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "\n",
        "    #dones_t = torch.BoolTensor(dones)\n",
        "    dones_t = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
        "    \n",
        "    #new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    new_states_t = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
        "\n",
        "    #return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "    #       dones_t.to(device),  new_states_t.to(device)\n",
        "    return states_t, actions_t, rewards_t, dones_t,  new_states_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spsqJAktfDU9"
      },
      "source": [
        "A not-so-easy piece of code, even being quite small, is the one to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN): # -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "    q_values = net(states_t)\n",
        "\n",
        "    indices = tf.stack([tf.range(batch_size), actions_t], axis=1)\n",
        "    state_action_values = tf.expand_dims(q_values, indices)\n",
        "\n",
        "    next_q = tf.reduce_max(tgt_net(new_states_t), axis=1)\n",
        "    next_q = next_q * tf.cast(tf.logical_not(dones_t), tf.float32)\n",
        "\n",
        "    expected_state_action_values = next_q * GAMMA + rewards_t\n",
        "\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    loss = loss_fn(expected_state_action_values, state_action_values)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "TwIK6i2ifNMj"
      },
      "outputs": [],
      "source": [
        "# def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "#               device: torch.device) -> torch.Tensor:\n",
        "#     states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "#     state_action_values = net(states_t).gather(\n",
        "#         1, actions_t.unsqueeze(-1)\n",
        "#     ).squeeze(-1) # squeeze retira uma dimensão de [[...]] para [...]\n",
        "#     with torch.no_grad():\n",
        "#         next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "#         next_state_values[dones_t] = 0.0\n",
        "#         next_state_values = next_state_values.detach() # não calcular gradientes\n",
        "\n",
        "#     expected_state_action_values = next_state_values * GAMMA + rewards_t\n",
        "#     return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84nZVAyFgCw6"
      },
      "source": [
        "Finally, we have the code for the training loop. Note that this training procedure is quite expensive it will be unfeaseble without the access to a GPU.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 84, 84)\n",
            "(4, 84, 84)\n",
            "<DQN name=dqn_2, built=False>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "2025-06-29 01:35:47.128640: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2025-06-29 01:35:47.129850: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
            "2025-06-29 01:35:47.129891: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: nixos-hyrulecastle\n",
            "2025-06-29 01:35:47.130518: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: nixos-hyrulecastle\n",
            "2025-06-29 01:35:47.132174: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program. The library may be missing or provided via another object.\n",
            "2025-06-29 01:35:47.136280: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 570.153.2\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Agent' object has no attribute 'dqn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m frame_idx += \u001b[32m1\u001b[39m\n\u001b[32m     42\u001b[39m epsilon = \u001b[38;5;28mmax\u001b[39m(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m reward = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m     total_rewards.append(reward)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mAgent.play_step\u001b[39m\u001b[34m(self, net, device, epsilon)\u001b[39m\n\u001b[32m     25\u001b[39m state_v = tf.expand_dims(state_v, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m#q_vals_v = net(state_v) # gera os Qs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m q_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdqn\u001b[49m(tf_state_batch)  \n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#_, act_v = torch.max(q_vals_v, dim=1) # queremos os maiores na dimensão 1\u001b[39;00m\n\u001b[32m     31\u001b[39m act_v = tf.argmax(q_values, axis=\u001b[32m1\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'Agent' object has no attribute 'dqn'"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import argparse\n",
        "import collections\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "                    help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "#env = wrappers.make_env(args.env)\n",
        "env = make_env(args.env)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "\n",
        "#writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "              f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "\n",
        "        # TODO do tensorboard writer\n",
        "        #writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        #writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        #writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        #writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "\n",
        "        if best_m_reward is None or best_m_reward < m_reward:\n",
        "            #torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "            net.save_weights(args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "        if m_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    # copy weights from net to tgt_net\n",
        "    #if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "    #    tgt_net.load_state_dict(net.state_dict())\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.set_weights(net.get_weights())\n",
        "\n",
        "    # optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "    if frame_idx % 10 == 0:\n",
        "        print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-9QeKX_f13p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "860: done 1 games, reward -21.000, eps 0.99, speed 612.75 f/s\n",
            "1742: done 2 games, reward -21.000, eps 0.99, speed 705.01 f/s\n",
            "2532: done 3 games, reward -21.000, eps 0.98, speed 609.66 f/s\n",
            "3442: done 4 games, reward -21.000, eps 0.98, speed 368.62 f/s\n",
            "4407: done 5 games, reward -21.000, eps 0.97, speed 523.28 f/s\n",
            "5327: done 6 games, reward -21.000, eps 0.96, speed 550.97 f/s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m frame_idx += \u001b[32m1\u001b[39m\n\u001b[32m     39\u001b[39m epsilon = \u001b[38;5;28mmax\u001b[39m(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m reward = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     43\u001b[39m     total_rewards.append(reward)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mAgent.play_step\u001b[39m\u001b[34m(self, net, device, epsilon)\u001b[39m\n\u001b[32m     20\u001b[39m state_v = torch.as_tensor(\u001b[38;5;28mself\u001b[39m.state).to(device)\n\u001b[32m     21\u001b[39m state_v.unsqueeze_(\u001b[32m0\u001b[39m) \u001b[38;5;66;03m# gera uma dimensão, erro comum\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m q_vals_v = \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_v\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# gera os Qs\u001b[39;00m\n\u001b[32m     23\u001b[39m _, act_v = torch.max(q_vals_v, dim=\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# queremos os maiores na dimensão 1\u001b[39;00m\n\u001b[32m     24\u001b[39m action = \u001b[38;5;28mint\u001b[39m(act_v.item()) \u001b[38;5;66;03m# faz a ação que tem o melhor Q\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mDQN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.ByteTensor):\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# scale on GPU\u001b[39;00m\n\u001b[32m     30\u001b[39m     xx = x / \u001b[32m255.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# import cv2\n",
        "# import argparse\n",
        "# import collections\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "# parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "#                     help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "# args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "# #env = wrappers.make_env(args.env)\n",
        "# env = make_env(args.env)\n",
        "# net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "# print(net)\n",
        "\n",
        "# buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "# agent = Agent(env, buffer)\n",
        "# epsilon = EPSILON_START\n",
        "\n",
        "# optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "# total_rewards = []\n",
        "# frame_idx = 0\n",
        "# ts_frame = 0\n",
        "# ts = time.time()\n",
        "# best_m_reward = None\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     frame_idx += 1\n",
        "#     epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "#     reward = agent.play_step(net, device, epsilon)\n",
        "#     if reward is not None:\n",
        "#         total_rewards.append(reward)\n",
        "#         speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "#         ts_frame = frame_idx\n",
        "#         ts = time.time()\n",
        "#         m_reward = np.mean(total_rewards[-100:])\n",
        "#         print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "#               f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "#         writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "#         writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "#         writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "#         writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "#         if best_m_reward is None or best_m_reward < m_reward:\n",
        "#             torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "#             if best_m_reward is not None:\n",
        "#                 print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "#             best_m_reward = m_reward\n",
        "#         if m_reward > MEAN_REWARD_BOUND:\n",
        "#             print(\"Solved in %d frames!\" % frame_idx)\n",
        "#             break\n",
        "#     if len(buffer) < REPLAY_START_SIZE:\n",
        "#         continue\n",
        "#     if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "#         tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "#     optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "#     batch = buffer.sample(BATCH_SIZE)\n",
        "#     loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "#     loss_t.backward()\n",
        "#     optimizer.step()\n",
        "#     if frame_idx % 10 == 0:\n",
        "#         print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "# writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgrTii2EhdQZ"
      },
      "source": [
        "The evolution of our agent's ability to play pong is recorded in the following collection of videos\n",
        "https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NdwRG-rhvzl"
      },
      "source": [
        "# Worksheet #9: Rewrite Laplan's code using TensorFlow and Keras."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
