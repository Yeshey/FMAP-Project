{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_yxCfjvnS_I"
      },
      "source": [
        "# Deep Q Learning.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9p2ibhhp-ta"
      },
      "source": [
        "If the number of states becomes too large, tabular learning becomes unfeasible. A way to circumvent this problem is to model $Q$ as a deep neural network that receives as inputs the state and the action, and outputs the corresponding $Q$ value. The parameters of this network can then be learned via gradient descent by considering a loss that measures how far $Q$ is from satisfying Bellman's equation.\n",
        "\n",
        "This is the basic conceptual idea behind the major breakthrough made in 2013 by a team from DeepMind https://arxiv.org/abs/1312.5602, which allowed them to train a DQN (Deep Q Network) to play several Atari games; in fact, the networke learned \"by itself\" starting from the pixel data of the frames of the computer games. This remarkable breakthrough was the starting point to a spectacular series of achievements that included mastering high-complexity strategy games such as chess and Go, and most importantly, the computation of the 3D structure of proteins https://www.nobelprize.org/prizes/chemistry/2024/summary/.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AappnB_LtXdi"
      },
      "source": [
        "A naive version of the DQN algorithm allude to above, can be summarizes as follows:\n",
        "\n",
        "1. Initialize $Q(s, a)$ using a standard deep learning initialization.\n",
        "\n",
        "2. By interacting with the environment, obtain the tuple $(s, a, r, s')$.\n",
        "\n",
        "3. Compute the loss:\n",
        "\n",
        "$${\\cal L} =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " (Q(s, a) - r)^2  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " (Q(s, a) - (r + \\gamma \\max_{a'} Q(s' ,a'))^2 \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "4. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "5. Repeat from step 2 until convergence (in the sense described for tabular-$Q$ learning) is achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCs21Aj2b50h"
      },
      "source": [
        "We'll now focus on a single Atari game, the iconic Pong https://ale.farama.org/environments/pong/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PhOeajsRcnXZ",
        "outputId": "7d16ec35-c3b7-496b-f702-ac621af9973c"
      },
      "outputs": [],
      "source": [
        "# !pip install 'stable_baselines3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "MZeLCmSQb5IR",
        "outputId": "9f89fbec-7b4a-4b1c-9515-b08739f1c4cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-30 04:03:36.366720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751252616.496013  398400 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751252616.543882  398400 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1751252616.908635  398400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751252616.908661  398400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751252616.908663  398400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751252616.908665  398400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-06-30 04:03:36.946755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(210, 160, 3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.1+2750686)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]]], dtype=uint8)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "import ale_py\n",
        "\n",
        "# Download and install ROMs (cant make it work)\n",
        "#wget http://www.atarimania.com/roms/Roms.rar\n",
        "#unrar x Roms.rar\n",
        "#ale-import-roms ROMS/\n",
        "\n",
        "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "\n",
        "obs,_ = env.reset()\n",
        "\n",
        "print(obs.shape)\n",
        "\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSoytnibWas"
      },
      "source": [
        "Unfortunately, the previous version of DQN doesn't work very well. So we need to do perform some upgrades.\n",
        "\n",
        "- On one hand, we need to explore the environment (using random actions); on the other, we want to use the knowledge gained by the $Q$-function; this is the famous \"exploration versus exploitation dilemma\".  We will resolve this dilemma by introducing a parameter $\\epsilon\\in[0,1]$, that will decrease with the number of iterations, and will be used to decide, with probability $\\epsilon$, if the agent will take a random action or will choose the action prescribed by $Q$.\n",
        "\n",
        "- We will also introduce a **replay buffer**. This will store a \"large\" number of transitions $(s, a, r, s')$ that will be used to construct training data batches to update the parameters of $Q$ using gradient descent.\n",
        "\n",
        "- Finally, using $Q$ itself to generate the targets for the loss will make the training very unstable. To circumvent this problem, we will introduce another DQN $\\hat Q$, called the **target network**, that is periodically synchronized with the main $Q$ network, but otherwise remains unchanged for a given number of iterations.   \n",
        "\n",
        "o $\\epsilon$ vai dexendo e quando chega a 0.5, metade das vezes explora, metade das vezes faz a melhor ação\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zfjMdN1OO5S"
      },
      "source": [
        "Upgraded DQN algorithm:  \n",
        "\n",
        "1. Initialize parameters for $Q(s,a)$ and $\\hat Q(s,a)$,$\\;\\epsilon \\leftarrow 1.0$, and empty the replay buffer.\n",
        "\n",
        "2. With probability $\\epsilon$, select a random action $a$; otherwise, $a = \\text{argmax}_a Q(s, a)$.\n",
        "\n",
        "3. Execute action $a$ in an emulator and observe the reward, $r$, and the next state, $s'$.\n",
        "\n",
        "4. Store the transition $(s, a, r, s')$ in the replay buffer.\n",
        "\n",
        "5. Sample a random mini-batch of transitions from the replay buffer.\n",
        "\n",
        "6. For every transition in the buffer, calculate the target:\n",
        "\n",
        "$$\n",
        "y =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " r  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " r + \\gamma \\max_{a'} \\hat Q(s' ,a') \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "7. Calculate the loss: ${\\cal L}=(Q(s,a)-y)^2$.\n",
        "\n",
        "8. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "9. Repeat from step 2 until convergence (in the sense described before) is achieved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7UB14l5VB7d"
      },
      "source": [
        "We will now present the Lapan's implementation of this algorithm, as exposed in chapter 6 of his book; see also\n",
        "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition.\n",
        "\n",
        "Note that Lapan uses PyTorch to implement the DQNs. It is a quite instructive exercise to translate the code to tensorflow/keras.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpamMumdWN-r"
      },
      "source": [
        "# **Wrappers:** For efficiency and conceptual reasons, we need to preprocess the environment. For instance:\n",
        "\n",
        "- We can reduce the game frames by considering only a monochromatic version of them and a lower resolution. This is \"hidden\" in the *atari_wrappers.AtariWrapper* class. After applying this, our images will have a shape of $(84,84,1)$; recall that originally they had a shape equal to $(210,160,3)$. Note that this class takes care of a lot of more relevant preprocessing details (see Chapter 6 of Lapan's book for more details).   \n",
        "\n",
        "- This isn't the way PyTorch is designed to receive information; it expects the form (channels, height, width). The ImageToPyTorch wrapper, presented below, takes care of this.\n",
        "\n",
        "- The agent won't be able to learn how to play Pong if we only provide still images of the game. To learn how to play Pong, we need to learn about dynamics. So we need to pack a given number (n_steps) of consecutive images into a \"small video\" with n_steps frames. This is taken care of by the BufferWrapper (see below).\n",
        "\n",
        "Here is the code for the wrappers.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JlM3EYsxZFQg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gym dá no formato (H, W, C) que é o que o tensorflow espera\n",
        "# class ImageToPyTorch(gym.ObservationWrapper):\n",
        "#     def __init__(self, env):\n",
        "#         super(ImageToPyTorch, self).__init__(env)\n",
        "#         obs = self.observation_space\n",
        "#         assert isinstance(obs, gym.spaces.Box)\n",
        "#         assert len(obs.shape) == 3\n",
        "#         new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "#         self.observation_space = gym.spaces.Box(\n",
        "#             low=obs.low.min(), high=obs.high.max(),\n",
        "#             shape=new_shape, dtype=obs.dtype)\n",
        "#\n",
        "#    def observation(self, observation):\n",
        "#        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "import typing as tt\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper): \n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            #obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            # antes os channels tavam no inicio agora estao no fim, agora temos (H, W, C * n_steps) não (C * n_steps, H, W)\n",
        "            obs.low.repeat(n_steps, axis=-1), obs.high.repeat(n_steps, axis=-1),\n",
        "            dtype=obs.dtype)\n",
        "\n",
        "        # old_shape = obs.shape\n",
        "        # new_shape = (old_shape[0], old_shape[1], old_shape[2] * n_steps)\n",
        "        # new_obs = gym.spaces.Box(\n",
        "        #     low=np.repeat(obs.low, n_steps, axis=-1),\n",
        "        #     high=np.repeat(obs.high, n_steps, axis=-1),\n",
        "        #     shape=new_shape,\n",
        "        #     dtype=obs.dtype\n",
        "        # )\n",
        "\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen-1): # preencher o buffer com frames vazias\n",
        "            self.buffer.append(self.env.observation_space.low)\n",
        "        obs, extra = self.env.reset() # reset gym env\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(list(self.buffer), axis=-1) # concat along channel (last in the list)\n",
        "        #return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, **kwargs):\n",
        "    env = gym.make(env_name, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    #env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=4)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXLwfJHZO-X"
      },
      "source": [
        "For the DQN model, we will use a typical convolution network, with a conv base followed by a dense network. Importantly, instead of modeling $Q$ as a function of the pair $(s,a)$ that outputs the corresponding value, i.e.,\n",
        "\n",
        "$$Q: {\\cal S}\\times {\\cal A} \\rightarrow \\mathbb{R}\\;,$$\n",
        "\n",
        "where, for our environment, we have\n",
        "\n",
        "$${\\cal S}=\\mathbb{R}^{84\\times 84\\times 4}$$\n",
        "\n",
        "and\n",
        "\n",
        "$${\\cal A} =\\{0,1,2,3,4,5\\} \\subset \\mathbb{R}\\;,$$\n",
        "\n",
        "we will use a dual representation   \n",
        "\n",
        "$$Q: {\\cal S} \\rightarrow \\mathbb{R}^6\\;,$$\n",
        "\n",
        "that given the state, outputs the value for each possible action.\n",
        "\n",
        "The PyTorch code should be, by now, self-explanatory. Notice nonetheless that it requires a bit more work, than the one needed in Keras to set dimensions of the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "class DQN(tf.keras.Model):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "\n",
        "        print(f\"DQN input shape: {input_shape}\")\n",
        "\n",
        "        self.rescale = layers.Rescaling(1./255) # normalização aqui\n",
        "        self.conv1 = layers.Conv2D(filters=32, kernel_size=8, strides=4, activation=\"relu\", input_shape=input_shape)\n",
        "        self.conv2 = layers.Conv2D(filters=64, kernel_size=4, strides=2, activation=\"relu\")\n",
        "        self.conv3 = layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=\"relu\")\n",
        "        self.flatten  = layers.Flatten()\n",
        "\n",
        "        # fully connected\n",
        "        self.fc = layers.Dense(units=512, activation='relu')\n",
        "        self.out = layers.Dense(units=n_actions, activation = None)\n",
        "\n",
        "\n",
        "    def call(self, inputs): # requiered for subclasses of tf.keras.Model\n",
        "        x = self.rescale(inputs)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        q = self.out(x)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J6GBp08adVU0"
      },
      "outputs": [],
      "source": [
        "# # bom exercicio é traduzir este codigo de torch para keras com tensorflow\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class DQN(nn.Module):\n",
        "#     def __init__(self, input_shape, n_actions):\n",
        "#         super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "#         #__super__().__init\n",
        "\n",
        "#         self.conv = nn.Sequential(\n",
        "#             nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), # convolução\n",
        "#             nn.ReLU(), # função de ativação relu\n",
        "#             nn.Conv2d(32, 64, kernel_size=4, stride=2), # 32 é igual ao 32 a cima, o keras faz isso automatico\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Flatten(),\n",
        "#         )\n",
        "#         size = self.conv(torch.zeros(1, *input_shape)).size()[-1] # ta a aplicar o vetor de convolução a um vetor de zeros oara ver o tamanho dele (.size = .shape)\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(size, 512), # rede densa\n",
        "#             nn.ReLU(), # relu\n",
        "#             nn.Linear(512, n_actions) # mais uma densa\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x: torch.ByteTensor):\n",
        "#         # scale on GPU\n",
        "#         xx = x / 255.0\n",
        "#         return self.fc(self.conv(xx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFQhRa3ddnF"
      },
      "source": [
        "Next we define several parameters and variables and construct the class fro replay buffer.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aG1OXGH_dvxj"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0 # começamos com probabilidade 1 de fazer algo ao calhas\n",
        "EPSILON_FINAL = 0.01 # acabamos com probabilidade 0.01 de fazer algo ao calhas\n",
        "\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    tf.Tensor,           # current state (batch, H, W, C)\n",
        "    tf.Tensor,           # actions\n",
        "    tf.Tensor,               # rewards\n",
        "    tf.Tensor,           # done || trunc\n",
        "    tf.Tensor           # next state\n",
        "]\n",
        "# BatchTensors = Tuple[\n",
        "#     np.ndarray,   # current state batch, shape (B, H, W, C)\n",
        "#     np.ndarray,   # actions batch, shape (B,)\n",
        "#     np.ndarray,   # rewards batch, shape (B,)\n",
        "#     np.ndarray,   # done flags batch, shape (B,)\n",
        "#     np.ndarray    # next state batch, shape (B, H, W, C)\n",
        "# ]\n",
        "\n",
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MNCg8vueHfi"
      },
      "source": [
        "We now create the agent class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "zknt1frZeSeI",
        "outputId": "255354e7-1eca-4e81-fbb4-734308e83de9"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def play_step(self, net: DQN,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon: # com probabilidade epsilon joga ao calhas\n",
        "            action = env.action_space.sample()\n",
        "        else: # caso contrario usa a informação do Q\n",
        "\n",
        "            #state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v = tf.convert_to_tensor(self.state, dtype=tf.float32)\n",
        "\n",
        "            #state_v.unsqueeze_(0) # gera uma dimensão, erro comum\n",
        "            state_v = tf.expand_dims(state_v, axis=0)\n",
        "\n",
        "            #q_vals_v = net(state_v) # gera os Qs\n",
        "            q_values = net(state_v)  \n",
        "\n",
        "            #_, act_v = torch.max(q_vals_v, dim=1) # queremos os maiores na dimensão 1\n",
        "            act_v = tf.argmax(q_values, axis=1)\n",
        "\n",
        "            #action = int(act_v.item()) # faz a ação que tem o melhor Q\n",
        "            action = int(act_v[0])\n",
        "            # action = int(act_idx.numpy()[0])\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action) # joga essa ação\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience( #\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        # informação esta guardada na forma (0,a,r,done,s'). e o buffer é uma lista dessas coisas\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6pMmUrjeTzi"
      },
      "source": [
        "We will also need to transform our samples taken from the buffer into tensors that can be fed to our networks. This is achieved by this simple function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JSLXGUYWeWYH"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "#def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "def batch_to_tensors(batch: List[Experience]) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    \n",
        "    #states_t = torch.as_tensor(np.asarray(states))\n",
        "    states_t = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "\n",
        "    #actions_t = torch.LongTensor(actions)\n",
        "    actions_t = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "\n",
        "    #rewards_t = torch.FloatTensor(rewards)\n",
        "    rewards_t = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "\n",
        "    #dones_t = torch.BoolTensor(dones)\n",
        "    dones_t = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
        "    \n",
        "    #new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    new_states_t = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
        "\n",
        "    #return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "    #       dones_t.to(device),  new_states_t.to(device)\n",
        "    return states_t, actions_t, rewards_t, dones_t,  new_states_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spsqJAktfDU9"
      },
      "source": [
        "A not-so-easy piece of code, even being quite small, is the one to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN): # -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "    q_values = net(states_t)\n",
        "\n",
        "    #indices = tf.stack([tf.range(BATCH_SIZE), actions_t], axis=1)\n",
        "    # q_values has shape (B, n_actions)\n",
        "    batch_range = tf.range(tf.shape(q_values)[0], dtype=actions_t.dtype)   # shape (B,)\n",
        "    indices     = tf.stack([batch_range, actions_t], axis=1)               # shape (B,2)\n",
        "\n",
        "    #state_action_values = tf.expand_dims(q_values, indices)\n",
        "    state_action_values = tf.gather_nd(q_values, indices)\n",
        "\n",
        "    next_q = tf.reduce_max(tgt_net(new_states_t), axis=1)\n",
        "    next_q = next_q * tf.cast(tf.logical_not(dones_t), tf.float32)\n",
        "\n",
        "    expected_state_action_values = next_q * GAMMA + rewards_t\n",
        "\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    loss = loss_fn(expected_state_action_values, state_action_values)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TwIK6i2ifNMj"
      },
      "outputs": [],
      "source": [
        "# def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "#               device: torch.device) -> torch.Tensor:\n",
        "#     states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "#     state_action_values = net(states_t).gather(\n",
        "#         1, actions_t.unsqueeze(-1)\n",
        "#     ).squeeze(-1) # squeeze retira uma dimensão de [[...]] para [...]\n",
        "#     with torch.no_grad():\n",
        "#         next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "#         next_state_values[dones_t] = 0.0\n",
        "#         next_state_values = next_state_values.detach() # não calcular gradientes\n",
        "\n",
        "#     expected_state_action_values = next_state_values * GAMMA + rewards_t\n",
        "#     return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84nZVAyFgCw6"
      },
      "source": [
        "Finally, we have the code for the training loop. Note that this training procedure is quite expensive it will be unfeaseble without the access to a GPU.  \n",
        "\n",
        "Ver tensorboard com `tensorboard --logdir=logs  --port=6006`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment observation space: (84, 84, 4)\n",
            "DQN input shape: (84, 84, 4)\n",
            "DQN input shape: (84, 84, 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "I0000 00:00:1751252623.245956  398400 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4273 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dummy input shape: (1, 84, 84, 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-30 04:03:43.780757: W external/local_xla/xla/service/gpu/llvm_gpu_backend/default/nvptx_libdevice_path.cc:40] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
            "Searched for CUDA in the following directories:\n",
            "  ./cuda_sdk_lib\n",
            "  ipykernel_launcher.runfiles/cuda_nvcc\n",
            "  ipykern/cuda_nvcc\n",
            "  \n",
            "  /usr/local/cuda\n",
            "  /opt/cuda\n",
            "  /mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
            "  /mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
            "  /mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/platform/../../cuda\n",
            "  /mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/platform/../../../../../..\n",
            "  /mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/platform/../../../../../../..\n",
            "  .\n",
            "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
            "I0000 00:00:1751252624.411003  398400 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network architecture:\n",
            "1104: done 1 games, reward -19.000, eps 0.99, speed 1222.08 f/s\n",
            "2104: done 2 games, reward -19.000, eps 0.99, speed 792.40 f/s\n",
            "2866: done 3 games, reward -19.667, eps 0.98, speed 964.72 f/s\n",
            "3908: done 4 games, reward -19.500, eps 0.97, speed 997.05 f/s\n",
            "4749: done 5 games, reward -19.600, eps 0.97, speed 938.76 f/s\n",
            "5716: done 6 games, reward -19.833, eps 0.96, speed 989.14 f/s\n",
            "6628: done 7 games, reward -20.000, eps 0.96, speed 910.06 f/s\n",
            "7469: done 8 games, reward -20.125, eps 0.95, speed 806.46 f/s\n",
            "8231: done 9 games, reward -20.222, eps 0.95, speed 722.00 f/s\n",
            "9398: done 10 games, reward -19.900, eps 0.94, speed 737.64 f/s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-30 04:03:58.238416: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
            "2025-06-30 04:03:58.240699: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
            "2025-06-30 04:03:58.243251: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
            "error: libdevice not found at ./libdevice.10.bc\n",
            "2025-06-30 04:03:58.245332: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
            "error: libdevice not found at ./libdevice.10.bc\n",
            "2025-06-30 04:03:58.247613: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
            "error: libdevice not found at ./libdevice.10.bc\n",
            "2025-06-30 04:03:58.249689: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
            "error: libdevice not found at ./libdevice.10.bc\n",
            "2025-06-30 04:03:58.252497: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
            "error: libdevice not found at ./libdevice.10.bc\n",
            "2025-06-30 04:03:58.255484: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
            "error: libdevice not found at ./libdevice.10.bc\n",
            "error: libdevice not found at ./libdevice.10.bc\n",
            "error: libdevice not found at ./libdevice.10.bc\n",
            "2025-06-30 04:03:58.256002: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:227] INTERNAL: Generating device code failed.\n",
            "2025-06-30 04:03:58.257208: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
            "2025-06-30 04:03:58.257225: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: JIT compilation failed.\n"
          ]
        },
        {
          "ename": "UnknownError",
          "evalue": "{{function_node __wrapped__Pow_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Pow] name: ",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnknownError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m     loss_t = calc_loss(batch, net, tgt_net)\n\u001b[32m    104\u001b[39m grads = tape.gradient(loss_t, net.trainable_variables)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame_idx % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraining in frame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_t\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:463\u001b[39m, in \u001b[36mBaseOptimizer.apply_gradients\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[32m    462\u001b[39m     grads, trainable_variables = \u001b[38;5;28mzip\u001b[39m(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterations\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:527\u001b[39m, in \u001b[36mBaseOptimizer.apply\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    524\u001b[39m     grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[32m    526\u001b[39m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:593\u001b[39m, in \u001b[36mBaseOptimizer._backend_apply_gradients\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28mself\u001b[39m._apply_weight_decay(trainable_variables)\n\u001b[32m    592\u001b[39m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_ema:\n\u001b[32m    598\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_model_variables_moving_average(\n\u001b[32m    599\u001b[39m         \u001b[38;5;28mself\u001b[39m._trainable_variables\n\u001b[32m    600\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:120\u001b[39m, in \u001b[36mTFOptimizer._backend_update_step\u001b[39m\u001b[34m(self, grads, trainable_variables, learning_rate)\u001b[39m\n\u001b[32m    118\u001b[39m grads_and_vars = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[32m    119\u001b[39m grads_and_vars = \u001b[38;5;28mself\u001b[39m._all_reduce_sum_gradients(grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__internal__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistribute\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_distributed_tf_update_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[39m, in \u001b[36mmaybe_merge_call\u001b[39m\u001b[34m(fn, strategy, *args, **kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[33;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \u001b[33;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib.get_replica_context().merge_call(\n\u001b[32m     54\u001b[39m       fn, args=args, kwargs=kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:134\u001b[39m, in \u001b[36mTFOptimizer._distributed_tf_update_step\u001b[39m\u001b[34m(self, distribution, grads_and_vars, learning_rate)\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.update_step(grad, var, learning_rate)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[43mdistribution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextended\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[39m, in \u001b[36mStrategyExtendedV2.update\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3002\u001b[39m   fn = autograph.tf_convert(\n\u001b[32m   3003\u001b[39m       fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3004\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3007\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._replica_ctx_update(\n\u001b[32m   3008\u001b[39m       var, fn, args=args, kwargs=kwargs, group=group)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[39m, in \u001b[36m_DefaultDistributionExtended._update\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   4072\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[32m   4073\u001b[39m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[32m   4074\u001b[39m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4075\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[39m, in \u001b[36m_DefaultDistributionExtended._update_non_slot\u001b[39m\u001b[34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[39m\n\u001b[32m   4077\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[32m   4078\u001b[39m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[32m   4079\u001b[39m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[32m   4080\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[32m-> \u001b[39m\u001b[32m4081\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[32m   4083\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[39m, in \u001b[36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    595\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:131\u001b[39m, in \u001b[36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[39m\u001b[34m(var, grad, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_grad_to_update_var\u001b[39m(var, grad, learning_rate):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/optimizers/adam.py:107\u001b[39m, in \u001b[36mAdam.update_step\u001b[39m\u001b[34m(self, gradient, variable, learning_rate)\u001b[39m\n\u001b[32m    105\u001b[39m gradient = ops.cast(gradient, variable.dtype)\n\u001b[32m    106\u001b[39m local_step = ops.cast(\u001b[38;5;28mself\u001b[39m.iterations + \u001b[32m1\u001b[39m, variable.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m beta_1_power = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbeta_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_step\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m beta_2_power = ops.power(\n\u001b[32m    111\u001b[39m     ops.cast(\u001b[38;5;28mself\u001b[39m.beta_2, variable.dtype), local_step\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m m = \u001b[38;5;28mself\u001b[39m._momentums[\u001b[38;5;28mself\u001b[39m._get_variable_index(variable)]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/ops/numpy.py:6391\u001b[39m, in \u001b[36mpower\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m   6389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[32m   6390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Power().symbolic_call(x1, x2)\n\u001b[32m-> \u001b[39m\u001b[32m6391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py:2653\u001b[39m, in \u001b[36mpower\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m   2651\u001b[39m x1 = convert_to_tensor(x1, dtype)\n\u001b[32m   2652\u001b[39m x2 = convert_to_tensor(x2, dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2653\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6006\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6004\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6005\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6006\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mUnknownError\u001b[39m: {{function_node __wrapped__Pow_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Pow] name: "
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import argparse\n",
        "import collections\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "                    help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "#env = wrappers.make_env(args.env)\n",
        "env = make_env(args.env)\n",
        "print(f\"Environment observation space: {env.observation_space.shape}\")\n",
        "net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "# Actually initialize both models by calling them with dummy input\n",
        "dummy_input = tf.zeros((1,) + env.observation_space.shape, dtype=tf.float32)\n",
        "print(f\"Dummy input shape: {dummy_input.shape}\")\n",
        "net(dummy_input)  # This creates the weights\n",
        "tgt_net(dummy_input)  # This creates the weights\n",
        "\n",
        "\n",
        "log_dir = f\"logs/{args.env}_{int(time.time())}\"\n",
        "writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "#writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "print(\"Network architecture:\")\n",
        "#net.build((None,) + env.observation_space.shape)\n",
        "#net.summary()\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "              f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "\n",
        "        # TensorBoard logging for TensorFlow\n",
        "        with writer.as_default():\n",
        "            tf.summary.scalar(\"epsilon\", epsilon, step=frame_idx)\n",
        "            tf.summary.scalar(\"speed\", speed, step=frame_idx)\n",
        "            tf.summary.scalar(\"reward_100\", m_reward, step=frame_idx)\n",
        "            tf.summary.scalar(\"reward\", reward, step=frame_idx)\n",
        "            writer.flush()\n",
        "        #writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        #writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        #writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        #writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "\n",
        "        if best_m_reward is None or best_m_reward < m_reward:\n",
        "            #torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "            net.save_weights(args.env + \"-best_%.0f.dat\" % m_reward + \".weights.h5\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "        if m_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    # copy weights from net to tgt_net\n",
        "    #if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "    #    tgt_net.load_state_dict(net.state_dict())\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.set_weights(net.get_weights())\n",
        "\n",
        "    # optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "\n",
        "\n",
        "    #loss_t = calc_loss(batch, net, tgt_net)\n",
        "    #loss_t.backward()\n",
        "    #optimizer.step()\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_t = calc_loss(batch, net, tgt_net)\n",
        "    grads = tape.gradient(loss_t, net.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
        "\n",
        "    if frame_idx % 10 == 0:\n",
        "        print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-9QeKX_f13p"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import argparse\n",
        "# import collections\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "# parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "#                     help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "# args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "# #env = wrappers.make_env(args.env)\n",
        "# env = make_env(args.env)\n",
        "# net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "# print(net)\n",
        "\n",
        "# buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "# agent = Agent(env, buffer)\n",
        "# epsilon = EPSILON_START\n",
        "\n",
        "# optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "# total_rewards = []\n",
        "# frame_idx = 0\n",
        "# ts_frame = 0\n",
        "# ts = time.time()\n",
        "# best_m_reward = None\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     frame_idx += 1\n",
        "#     epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "#     reward = agent.play_step(net, device, epsilon)\n",
        "#     if reward is not None:\n",
        "#         total_rewards.append(reward)\n",
        "#         speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "#         ts_frame = frame_idx\n",
        "#         ts = time.time()\n",
        "#         m_reward = np.mean(total_rewards[-100:])\n",
        "#         print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "#               f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "#         writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "#         writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "#         writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "#         writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "#         if best_m_reward is None or best_m_reward < m_reward:\n",
        "#             torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "#             if best_m_reward is not None:\n",
        "#                 print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "#             best_m_reward = m_reward\n",
        "#         if m_reward > MEAN_REWARD_BOUND:\n",
        "#             print(\"Solved in %d frames!\" % frame_idx)\n",
        "#             break\n",
        "#     if len(buffer) < REPLAY_START_SIZE:\n",
        "#         continue\n",
        "#     if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "#         tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "#     optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "#     batch = buffer.sample(BATCH_SIZE)\n",
        "#     loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "#     loss_t.backward()\n",
        "#     optimizer.step()\n",
        "#     if frame_idx % 10 == 0:\n",
        "#         print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "# writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgrTii2EhdQZ"
      },
      "source": [
        "The evolution of our agent's ability to play pong is recorded in the following collection of videos\n",
        "https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
