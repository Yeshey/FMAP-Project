{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_yxCfjvnS_I"
      },
      "source": [
        "# Deep Q Learning.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9p2ibhhp-ta"
      },
      "source": [
        "If the number of states becomes too large, tabular learning becomes unfeasible. A way to circumvent this problem is to model $Q$ as a deep neural network that receives as inputs the state and the action, and outputs the corresponding $Q$ value. The parameters of this network can then be learned via gradient descent by considering a loss that measures how far $Q$ is from satisfying Bellman's equation.\n",
        "\n",
        "This is the basic conceptual idea behind the major breakthrough made in 2013 by a team from DeepMind https://arxiv.org/abs/1312.5602, which allowed them to train a DQN (Deep Q Network) to play several Atari games; in fact, the networke learned \"by itself\" starting from the pixel data of the frames of the computer games. This remarkable breakthrough was the starting point to a spectacular series of achievements that included mastering high-complexity strategy games such as chess and Go, and most importantly, the computation of the 3D structure of proteins https://www.nobelprize.org/prizes/chemistry/2024/summary/.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AappnB_LtXdi"
      },
      "source": [
        "A naive version of the DQN algorithm allude to above, can be summarizes as follows:\n",
        "\n",
        "1. Initialize $Q(s, a)$ using a standard deep learning initialization.\n",
        "\n",
        "2. By interacting with the environment, obtain the tuple $(s, a, r, s')$.\n",
        "\n",
        "3. Compute the loss:\n",
        "\n",
        "$${\\cal L} =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " (Q(s, a) - r)^2  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " (Q(s, a) - (r + \\gamma \\max_{a'} Q(s' ,a'))^2 \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "4. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "5. Repeat from step 2 until convergence (in the sense described for tabular-$Q$ learning) is achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCs21Aj2b50h"
      },
      "source": [
        "We'll now focus on a single Atari game, the iconic Pong https://ale.farama.org/environments/pong/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PhOeajsRcnXZ",
        "outputId": "7d16ec35-c3b7-496b-f702-ac621af9973c"
      },
      "outputs": [],
      "source": [
        "# !pip install 'stable_baselines3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "MZeLCmSQb5IR",
        "outputId": "9f89fbec-7b4a-4b1c-9515-b08739f1c4cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-01 11:55:12.887699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751367313.078643    8330 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751367313.120614    8330 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1751367313.447902    8330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751367313.447963    8330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751367313.447998    8330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751367313.448005    8330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-07-01 11:55:13.508806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(210, 160, 3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.1+2750686)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       [[109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        ...,\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43],\n",
              "        [109, 118,  43]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]],\n",
              "\n",
              "       [[ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        ...,\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24],\n",
              "        [ 53,  95,  24]]], dtype=uint8)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "import ale_py\n",
        "\n",
        "# Download and install ROMs (cant make it work)\n",
        "#wget http://www.atarimania.com/roms/Roms.rar\n",
        "#unrar x Roms.rar\n",
        "#ale-import-roms ROMS/\n",
        "\n",
        "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "\n",
        "obs,_ = env.reset()\n",
        "\n",
        "print(obs.shape)\n",
        "\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSoytnibWas"
      },
      "source": [
        "Unfortunately, the previous version of DQN doesn't work very well. So we need to do perform some upgrades.\n",
        "\n",
        "- On one hand, we need to explore the environment (using random actions); on the other, we want to use the knowledge gained by the $Q$-function; this is the famous \"exploration versus exploitation dilemma\".  We will resolve this dilemma by introducing a parameter $\\epsilon\\in[0,1]$, that will decrease with the number of iterations, and will be used to decide, with probability $\\epsilon$, if the agent will take a random action or will choose the action prescribed by $Q$.\n",
        "\n",
        "- We will also introduce a **replay buffer**. This will store a \"large\" number of transitions $(s, a, r, s')$ that will be used to construct training data batches to update the parameters of $Q$ using gradient descent.\n",
        "\n",
        "- Finally, using $Q$ itself to generate the targets for the loss will make the training very unstable. To circumvent this problem, we will introduce another DQN $\\hat Q$, called the **target network**, that is periodically synchronized with the main $Q$ network, but otherwise remains unchanged for a given number of iterations.   \n",
        "\n",
        "o $\\epsilon$ vai dexendo e quando chega a 0.5, metade das vezes explora, metade das vezes faz a melhor ação\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zfjMdN1OO5S"
      },
      "source": [
        "Upgraded DQN algorithm:  \n",
        "\n",
        "1. Initialize parameters for $Q(s,a)$ and $\\hat Q(s,a)$,$\\;\\epsilon \\leftarrow 1.0$, and empty the replay buffer.\n",
        "\n",
        "2. With probability $\\epsilon$, select a random action $a$; otherwise, $a = \\text{argmax}_a Q(s, a)$.\n",
        "\n",
        "3. Execute action $a$ in an emulator and observe the reward, $r$, and the next state, $s'$.\n",
        "\n",
        "4. Store the transition $(s, a, r, s')$ in the replay buffer.\n",
        "\n",
        "5. Sample a random mini-batch of transitions from the replay buffer.\n",
        "\n",
        "6. For every transition in the buffer, calculate the target:\n",
        "\n",
        "$$\n",
        "y =\n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        " r  \\quad,\\text{ if episode ended}\\;, \\\\\n",
        " r + \\gamma \\max_{a'} \\hat Q(s' ,a') \\quad, \\text{ otherwise} \\;.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "7. Calculate the loss: ${\\cal L}=(Q(s,a)-y)^2$.\n",
        "\n",
        "8. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
        "\n",
        "9. Repeat from step 2 until convergence (in the sense described before) is achieved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7UB14l5VB7d"
      },
      "source": [
        "We will now present the Lapan's implementation of this algorithm, as exposed in chapter 6 of his book; see also\n",
        "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition.\n",
        "\n",
        "Note that Lapan uses PyTorch to implement the DQNs. It is a quite instructive exercise to translate the code to tensorflow/keras.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpamMumdWN-r"
      },
      "source": [
        "# **Wrappers:** For efficiency and conceptual reasons, we need to preprocess the environment. For instance:\n",
        "\n",
        "- We can reduce the game frames by considering only a monochromatic version of them and a lower resolution. This is \"hidden\" in the *atari_wrappers.AtariWrapper* class. After applying this, our images will have a shape of $(84,84,1)$; recall that originally they had a shape equal to $(210,160,3)$. Note that this class takes care of a lot of more relevant preprocessing details (see Chapter 6 of Lapan's book for more details).   \n",
        "\n",
        "- This isn't the way PyTorch is designed to receive information; it expects the form (channels, height, width). The ImageToPyTorch wrapper, presented below, takes care of this.\n",
        "\n",
        "- The agent won't be able to learn how to play Pong if we only provide still images of the game. To learn how to play Pong, we need to learn about dynamics. So we need to pack a given number (n_steps) of consecutive images into a \"small video\" with n_steps frames. This is taken care of by the BufferWrapper (see below).\n",
        "\n",
        "Here is the code for the wrappers.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlM3EYsxZFQg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gym dá no formato (H, W, C) que é o que o tensorflow espera\n",
        "\n",
        "import typing as tt\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper): \n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            #obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            # antes os channels tavam no inicio agora estao no fim, agora temos (H, W, C * n_steps) não (C * n_steps, H, W)\n",
        "            obs.low.repeat(n_steps, axis=-1), obs.high.repeat(n_steps, axis=-1),\n",
        "            dtype=obs.dtype)\n",
        "\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen-1): # preencher o buffer com frames vazias\n",
        "            self.buffer.append(self.env.observation_space.low)\n",
        "        obs, extra = self.env.reset() # reset gym env\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(list(self.buffer), axis=-1) # concat along channel (last in the list)\n",
        "        #return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, **kwargs):\n",
        "    env = gym.make(env_name, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    #env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=4)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXLwfJHZO-X"
      },
      "source": [
        "For the DQN model, we will use a typical convolution network, with a conv base followed by a dense network. Importantly, instead of modeling $Q$ as a function of the pair $(s,a)$ that outputs the corresponding value, i.e.,\n",
        "\n",
        "$$Q: {\\cal S}\\times {\\cal A} \\rightarrow \\mathbb{R}\\;,$$\n",
        "\n",
        "where, for our environment, we have\n",
        "\n",
        "$${\\cal S}=\\mathbb{R}^{84\\times 84\\times 4}$$\n",
        "\n",
        "and\n",
        "\n",
        "$${\\cal A} =\\{0,1,2,3,4,5\\} \\subset \\mathbb{R}\\;,$$\n",
        "\n",
        "we will use a dual representation   \n",
        "\n",
        "$$Q: {\\cal S} \\rightarrow \\mathbb{R}^6\\;,$$\n",
        "\n",
        "that given the state, outputs the value for each possible action.\n",
        "\n",
        "The PyTorch code should be, by now, self-explanatory. Notice nonetheless that it requires a bit more work, than the one needed in Keras to set dimensions of the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available: 1\n",
            "Is built with cuda: True\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "print(\"Is built with cuda:\", tf.test.is_built_with_cuda())\n",
        "\n",
        "class DQN(tf.keras.Model):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "\n",
        "        print(f\"DQN input shape: {input_shape}\")\n",
        "\n",
        "        self.rescale = layers.Rescaling(1./255) # normalização aqui\n",
        "        self.conv1 = layers.Conv2D(filters=32, kernel_size=8, strides=4, activation=\"relu\", input_shape=input_shape)\n",
        "        self.conv2 = layers.Conv2D(filters=64, kernel_size=4, strides=2, activation=\"relu\")\n",
        "        self.conv3 = layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=\"relu\")\n",
        "        self.flatten  = layers.Flatten()\n",
        "\n",
        "        # fully connected\n",
        "        self.fc = layers.Dense(units=512, activation='relu')\n",
        "        self.out = layers.Dense(units=n_actions, activation = None)\n",
        "\n",
        "\n",
        "    def call(self, inputs): # requiered for subclasses of tf.keras.Model\n",
        "        x = self.rescale(inputs)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        q = self.out(x)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6GBp08adVU0"
      },
      "outputs": [],
      "source": [
        "# # bom exercicio é traduzir este codigo de torch para keras com tensorflow\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class DQN(nn.Module):\n",
        "#     def __init__(self, input_shape, n_actions):\n",
        "#         super(DQN, self).__init__() # ta a fazer init da superclass nn.Module\n",
        "#         #__super__().__init\n",
        "\n",
        "#         self.conv = nn.Sequential(\n",
        "#             nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), # convolução\n",
        "#             nn.ReLU(), # função de ativação relu\n",
        "#             nn.Conv2d(32, 64, kernel_size=4, stride=2), # 32 é igual ao 32 a cima, o keras faz isso automatico\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Flatten(),\n",
        "#         )\n",
        "#         size = self.conv(torch.zeros(1, *input_shape)).size()[-1] # ta a aplicar o vetor de convolução a um vetor de zeros oara ver o tamanho dele (.size = .shape)\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(size, 512), # rede densa\n",
        "#             nn.ReLU(), # relu\n",
        "#             nn.Linear(512, n_actions) # mais uma densa\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x: torch.ByteTensor):\n",
        "#         # scale on GPU\n",
        "#         xx = x / 255.0\n",
        "#         return self.fc(self.conv(xx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFQhRa3ddnF"
      },
      "source": [
        "Next we define several parameters and variables and construct the class fro replay buffer.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG1OXGH_dvxj"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0 # começamos com probabilidade 1 de fazer algo ao calhas\n",
        "EPSILON_FINAL = 0.01 # acabamos com probabilidade 0.01 de fazer algo ao calhas\n",
        "\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    tf.Tensor,           # current state (batch, H, W, C)\n",
        "    tf.Tensor,           # actions\n",
        "    tf.Tensor,               # rewards\n",
        "    tf.Tensor,           # done || trunc\n",
        "    tf.Tensor           # next state\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MNCg8vueHfi"
      },
      "source": [
        "We now create the agent class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "zknt1frZeSeI",
        "outputId": "255354e7-1eca-4e81-fbb4-734308e83de9"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def play_step(self, net: DQN,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon: # com probabilidade epsilon joga ao calhas\n",
        "            action = env.action_space.sample()\n",
        "        else: # caso contrario usa a informação do Q\n",
        "\n",
        "            #state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v = tf.convert_to_tensor(self.state, dtype=tf.float32)\n",
        "\n",
        "            #state_v.unsqueeze_(0) # gera uma dimensão, erro comum\n",
        "            state_v = tf.expand_dims(state_v, axis=0)\n",
        "\n",
        "            #q_vals_v = net(state_v) # gera os Qs\n",
        "            q_values = net(state_v)  \n",
        "\n",
        "            #_, act_v = torch.max(q_vals_v, dim=1) # queremos os maiores na dimensão 1\n",
        "            act_v = tf.argmax(q_values, axis=1)\n",
        "\n",
        "            #action = int(act_v.item()) # faz a ação que tem o melhor Q\n",
        "            action = int(act_v[0])\n",
        "            # action = int(act_idx.numpy()[0])\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action) # joga essa ação\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience( #\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        # informação esta guardada na forma (0,a,r,done,s'). e o buffer é uma lista dessas coisas\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6pMmUrjeTzi"
      },
      "source": [
        "We will also need to transform our samples taken from the buffer into tensors that can be fed to our networks. This is achieved by this simple function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JSLXGUYWeWYH"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "#def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "def batch_to_tensors(batch: List[Experience]) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    \n",
        "    #states_t = torch.as_tensor(np.asarray(states))\n",
        "    states_t = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "\n",
        "    #actions_t = torch.LongTensor(actions)\n",
        "    actions_t = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "\n",
        "    #rewards_t = torch.FloatTensor(rewards)\n",
        "    rewards_t = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "\n",
        "    #dones_t = torch.BoolTensor(dones)\n",
        "    dones_t = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
        "    \n",
        "    #new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    new_states_t = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
        "\n",
        "    #return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "    #       dones_t.to(device),  new_states_t.to(device)\n",
        "    return states_t, actions_t, rewards_t, dones_t,  new_states_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spsqJAktfDU9"
      },
      "source": [
        "A not-so-easy piece of code, even being quite small, is the one to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN): # -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "    q_values = net(states_t)\n",
        "\n",
        "    #indices = tf.stack([tf.range(BATCH_SIZE), actions_t], axis=1)\n",
        "    # q_values has shape (B, n_actions)\n",
        "    batch_range = tf.range(tf.shape(q_values)[0], dtype=actions_t.dtype)   # shape (B,)\n",
        "    indices     = tf.stack([batch_range, actions_t], axis=1)               # shape (B,2)\n",
        "\n",
        "    #state_action_values = tf.expand_dims(q_values, indices)\n",
        "    state_action_values = tf.gather_nd(q_values, indices)\n",
        "\n",
        "    next_q = tf.reduce_max(tgt_net(new_states_t), axis=1)\n",
        "    next_q = next_q * tf.cast(tf.logical_not(dones_t), tf.float32)\n",
        "\n",
        "    expected_state_action_values = next_q * GAMMA + rewards_t\n",
        "\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    loss = loss_fn(expected_state_action_values, state_action_values)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TwIK6i2ifNMj"
      },
      "outputs": [],
      "source": [
        "# def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "#               device: torch.device) -> torch.Tensor:\n",
        "#     states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch)\n",
        "\n",
        "#     state_action_values = net(states_t).gather(\n",
        "#         1, actions_t.unsqueeze(-1)\n",
        "#     ).squeeze(-1) # squeeze retira uma dimensão de [[...]] para [...]\n",
        "#     with torch.no_grad():\n",
        "#         next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "#         next_state_values[dones_t] = 0.0\n",
        "#         next_state_values = next_state_values.detach() # não calcular gradientes\n",
        "\n",
        "#     expected_state_action_values = next_state_values * GAMMA + rewards_t\n",
        "#     return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84nZVAyFgCw6"
      },
      "source": [
        "Finally, we have the code for the training loop. Note that this training procedure is quite expensive it will be unfeaseble without the access to a GPU.  \n",
        "\n",
        "Ver tensorboard com `tensorboard --logdir=logs  --port=6006`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment observation space: (84, 84, 4)\n",
            "DQN input shape: (84, 84, 4)\n",
            "DQN input shape: (84, 84, 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "I0000 00:00:1751367323.572223    8330 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4247 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dummy input shape: (1, 84, 84, 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1751367324.979340    8330 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network architecture:\n",
            "896: done 1 games, reward -20.000, eps 0.99, speed 1200.66 f/s\n",
            "1875: done 2 games, reward -19.500, eps 0.99, speed 809.05 f/s\n",
            "Best reward updated -20.000 -> -19.500\n",
            "2697: done 3 games, reward -20.000, eps 0.98, speed 977.98 f/s\n",
            "3478: done 4 games, reward -20.250, eps 0.98, speed 1028.19 f/s\n",
            "4298: done 5 games, reward -20.400, eps 0.97, speed 1027.87 f/s\n",
            "5487: done 6 games, reward -20.167, eps 0.96, speed 965.24 f/s\n",
            "6496: done 7 games, reward -20.000, eps 0.96, speed 938.26 f/s\n",
            "7411: done 8 games, reward -20.000, eps 0.95, speed 796.35 f/s\n",
            "8444: done 9 games, reward -19.889, eps 0.94, speed 857.81 f/s\n",
            "9234: done 10 games, reward -20.000, eps 0.94, speed 843.16 f/s\n",
            "training in frame 10000, Loss = 0.0164\n",
            "training in frame 10010, Loss = 0.0318\n",
            "training in frame 10020, Loss = 0.0007\n",
            "training in frame 10030, Loss = 0.0015\n",
            "training in frame 10040, Loss = 0.0615\n",
            "training in frame 10050, Loss = 0.0009\n",
            "training in frame 10060, Loss = 0.0305\n",
            "training in frame 10070, Loss = 0.0665\n",
            "training in frame 10080, Loss = 0.0312\n",
            "training in frame 10090, Loss = 0.0004\n",
            "training in frame 10100, Loss = 0.0645\n",
            "training in frame 10110, Loss = 0.0003\n",
            "training in frame 10120, Loss = 0.0317\n",
            "training in frame 10130, Loss = 0.0292\n",
            "training in frame 10140, Loss = 0.0621\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m#loss_t = calc_loss(batch, net, tgt_net)\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m#loss_t.backward()\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m#optimizer.step()\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.GradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     loss_t = \u001b[43mcalc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_net\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m grads = tape.gradient(loss_t, net.trainable_variables)\n\u001b[32m    105\u001b[39m optimizer.apply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, net.trainable_variables))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mcalc_loss\u001b[39m\u001b[34m(batch, net, tgt_net)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_loss\u001b[39m(batch: tt.List[Experience], net: DQN, tgt_net: DQN): \u001b[38;5;66;03m# -> torch.Tensor:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     states_t, actions_t, rewards_t, dones_t, new_states_t = \u001b[43mbatch_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     q_values = net(states_t)\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m#indices = tf.stack([tf.range(BATCH_SIZE), actions_t], axis=1)\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# q_values has shape (B, n_actions)\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mbatch_to_tensors\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     23\u001b[39m dones_t = tf.convert_to_tensor(dones, dtype=tf.bool)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#new_states_t = torch.as_tensor(np.asarray(new_state))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m new_states_t = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#       dones_t.to(device),  new_states_t.to(device)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m states_t, actions_t, rewards_t, dones_t,  new_states_t\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion.py:161\u001b[39m, in \u001b[36mconvert_to_tensor_v2_with_dispatch\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m.tf_export(\u001b[33m\"\u001b[39m\u001b[33mconvert_to_tensor\u001b[39m\u001b[33m\"\u001b[39m, v1=[])\n\u001b[32m     97\u001b[39m \u001b[38;5;129m@dispatch\u001b[39m.add_dispatch_support\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[32m     99\u001b[39m     value, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    100\u001b[39m ) -> tensor_lib.Tensor:\n\u001b[32m    101\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[32m    102\u001b[39m \n\u001b[32m    103\u001b[39m \u001b[33;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion.py:171\u001b[39m, in \u001b[36mconvert_to_tensor_v2\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    225\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    226\u001b[39m           _add_error_prefix(\n\u001b[32m    227\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret.dtype.base_dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    231\u001b[39m               name=name))\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m   ret = \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    237\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[39m, in \u001b[36m_constant_tensor_conversion_function\u001b[39m\u001b[34m(v, dtype, name, as_ref)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[32m     28\u001b[39m _ = as_ref\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[39m, in \u001b[36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    141\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m   bound_arguments.apply_defaults()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[39m, in \u001b[36mconstant\u001b[39m\u001b[34m(value, dtype, shape, name)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mconstant\u001b[39m\u001b[33m\"\u001b[39m, v1=[])\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconstant\u001b[39m(\n\u001b[32m    179\u001b[39m     value, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, shape=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[33m\"\u001b[39m\u001b[33mConst\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m ) -> Union[ops.Operation, ops._EagerTensorBase]:\n\u001b[32m    181\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[32m    182\u001b[39m \n\u001b[32m    183\u001b[39m \u001b[33;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m \u001b[33;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[39m, in \u001b[36m_constant_impl\u001b[39m\u001b[34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[39m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(\u001b[33m\"\u001b[39m\u001b[33mtf.constant\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    288\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m const_tensor = ops._create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    292\u001b[39m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[32m    293\u001b[39m )\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[39m, in \u001b[36m_constant_eager_impl\u001b[39m\u001b[34m(ctx, value, dtype, shape, verify_shape)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_constant_eager_impl\u001b[39m(\n\u001b[32m    298\u001b[39m     ctx, value, dtype, shape, verify_shape\n\u001b[32m    299\u001b[39m ) -> ops._EagerTensorBase:\n\u001b[32m    300\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m   t = \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/DataDisk/PersonalFiles/2025/Masters/FMAP -  Fundamentos Matemáticos para Aprendizagem Profunda/FMAP-Project/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[39m, in \u001b[36mconvert_to_eager_tensor\u001b[39m\u001b[34m(value, ctx, dtype)\u001b[39m\n\u001b[32m    106\u001b[39m     dtype = dtypes.as_dtype(dtype).as_datatype_enum\n\u001b[32m    107\u001b[39m ctx.ensure_initialized()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import argparse\n",
        "import collections\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "                    help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "#env = wrappers.make_env(args.env)\n",
        "env = make_env(args.env)\n",
        "print(f\"Environment observation space: {env.observation_space.shape}\")\n",
        "net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "# initialize both models weights by calling them with dummy input\n",
        "dummy_input = tf.zeros((1,) + env.observation_space.shape, dtype=tf.float32)\n",
        "print(f\"Dummy input shape: {dummy_input.shape}\")\n",
        "net(dummy_input)  # This creates the weights\n",
        "tgt_net(dummy_input)  # This creates the weights\n",
        "\n",
        "\n",
        "log_dir = f\"logs/{args.env}_{int(time.time())}\"\n",
        "writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "#writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "print(\"Network architecture:\")\n",
        "#net.build((None,) + env.observation_space.shape)\n",
        "#net.summary()\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "              f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "\n",
        "        # TensorBoard logging for TensorFlow\n",
        "        with writer.as_default():\n",
        "            tf.summary.scalar(\"epsilon\", epsilon, step=frame_idx)\n",
        "            tf.summary.scalar(\"speed\", speed, step=frame_idx)\n",
        "            tf.summary.scalar(\"reward_100\", m_reward, step=frame_idx)\n",
        "            tf.summary.scalar(\"reward\", reward, step=frame_idx)\n",
        "            writer.flush()\n",
        "        #writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        #writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        #writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        #writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "\n",
        "        if best_m_reward is None or best_m_reward < m_reward:\n",
        "            #torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "            net.save_weights(args.env + \"-best_%.0f.dat\" % m_reward + \".weights.h5\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "        if m_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    # copy weights from net to tgt_net\n",
        "    #if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "    #    tgt_net.load_state_dict(net.state_dict())\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.set_weights(net.get_weights())\n",
        "\n",
        "    # optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "\n",
        "    #loss_t = calc_loss(batch, net, tgt_net)\n",
        "    #loss_t.backward()\n",
        "    #optimizer.step()\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_t = calc_loss(batch, net, tgt_net)\n",
        "    grads = tape.gradient(loss_t, net.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
        "\n",
        "    if frame_idx % 10 == 0:\n",
        "        print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-9QeKX_f13p"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import argparse\n",
        "# import collections\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
        "# parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "#                     help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "# args, _ = parser.parse_known_args()\n",
        "# device = torch.device(args.dev)\n",
        "\n",
        "# #env = wrappers.make_env(args.env)\n",
        "# env = make_env(args.env)\n",
        "# net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "# print(net)\n",
        "\n",
        "# buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "# agent = Agent(env, buffer)\n",
        "# epsilon = EPSILON_START\n",
        "\n",
        "# optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "# total_rewards = []\n",
        "# frame_idx = 0\n",
        "# ts_frame = 0\n",
        "# ts = time.time()\n",
        "# best_m_reward = None\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     frame_idx += 1\n",
        "#     epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "#     reward = agent.play_step(net, device, epsilon)\n",
        "#     if reward is not None:\n",
        "#         total_rewards.append(reward)\n",
        "#         speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "#         ts_frame = frame_idx\n",
        "#         ts = time.time()\n",
        "#         m_reward = np.mean(total_rewards[-100:])\n",
        "#         print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "#               f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "#         writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "#         writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "#         writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "#         writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "#         if best_m_reward is None or best_m_reward < m_reward:\n",
        "#             torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "#             if best_m_reward is not None:\n",
        "#                 print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "#             best_m_reward = m_reward\n",
        "#         if m_reward > MEAN_REWARD_BOUND:\n",
        "#             print(\"Solved in %d frames!\" % frame_idx)\n",
        "#             break\n",
        "#     if len(buffer) < REPLAY_START_SIZE:\n",
        "#         continue\n",
        "#     if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "#         tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "#     optimizer.zero_grad() # precisamos de fazer manualmente, se não acumulam, em tensorflow keras n temos\n",
        "#     batch = buffer.sample(BATCH_SIZE)\n",
        "#     loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "#     loss_t.backward()\n",
        "#     optimizer.step()\n",
        "#     if frame_idx % 10 == 0:\n",
        "#         print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
        "# writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgrTii2EhdQZ"
      },
      "source": [
        "The evolution of our agent's ability to play pong is recorded in the following collection of videos\n",
        "https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
