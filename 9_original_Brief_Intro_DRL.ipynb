{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBBdgAwBY6yU"
   },
   "source": [
    "# A very brief introduction to Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEwBRJqcZRIS"
   },
   "source": [
    "Consider an **agent** whose goal is to solve a given problem: finding its way through a maze, finding the best way to solve a logistics problem, learning how to play games (Atari or other digital games, or strategy games like Chess and Go), finding the 3d dimensional structure of proteins, or turning a Large Language Model from an incoherent and rude \"blabbermouth\" into a coherent an polite language generator.\n",
    "\n",
    "In Reinforcement Learning (RL) such problems need to be solved by exploring a given environment: the agent is able to probe the environment by observing its states (that might only contain partial information) and by using a set of actions that (typically) change the state of the environment; moreover, the agent receives stimuli from the environment, in the form of rewards, that depend on the chosen action and state achieved.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2WlC7c3hcXo"
   },
   "source": [
    "To simplify things, let's only consider the case where there is only a finite **observation space**, i.e., a finite set of states\n",
    "\n",
    "$${\\cal S} = \\{s_1,\\ldots,s_N\\}\\;,$$\n",
    "\n",
    "and a finite set of actions, independent of the state,\n",
    "\n",
    "$${\\cal A}=\\{a_1,\\ldots,a_M\\}\\;.$$\n",
    "\n",
    "We will also assume that by taking action $a_i$ in state $s_j$ the agent gets a reward\n",
    "\n",
    "$$R(a_i,s_j) \\in \\mathbb{R}\\;.$$\n",
    "\n",
    "Each agent acts according to a **policy**, that corresponds to a choice of action given the observed state\n",
    "\n",
    "$$\\pi: {\\cal S}\\rightarrow {\\cal A}\\;.$$\n",
    "\n",
    "We will consider discrete time, finite horizon games, i.e., games where the actions are taken in separate moments in time and finish at time $T\\in \\mathbb{Z}^+$. In such a case we can take the sequence of moments to be $\\{0,1,\\ldots,T\\}$, and given a policy $\\pi$ the agent will receive a sequence of rewards $\\{R_{\\pi}(0),\\ldots, R_{\\pi}(T)\\}$ with a combined **discounted return** of\n",
    "\n",
    "$$G_{\\pi} = R_{\\pi}(0)+ \\gamma R_{\\pi}(1) + \\gamma^2 R_{\\pi}(2) + \\cdots + \\gamma^T R_{\\pi}(T) = \\sum_{t=0}^{T} \\gamma^t R_{\\pi}(t)\\;,$$\n",
    "\n",
    "where we have included a discount factor $0<\\gamma \\leq 1$. If $\\gamma < 1$, the goal of the discount factor is to make future rewards less relevant to the agent's decisions at times $t< T$.   \n",
    "\n",
    "Naturally, the goal of the agent is to find an optimal policy $\\pi^*$, i.e., a policy that maximizes the return\n",
    "\n",
    "$$G_{\\pi^*} = \\max_{\\pi} G_{\\pi}\\,.$$\n",
    "\n",
    "This setting can be generalized to a stochastic context, where taking a given action does not lead to a determined (next) state but instead gives rise to a probability distribution over the states. In such a case, the rewards become random variables, and the return is then given by an expectation value. To simplify our presentation, we'll not formalize this important generalization.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yucOHj7-Qzm"
   },
   "source": [
    "We'll now explore some of the previous concepts in Gymnasium \"An API standard for reinforcement learning with a diverse collection of reference environments\"\n",
    "https://gymnasium.farama.org/index.html\n",
    "\n",
    "To start let's consider Frozen Lake https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DBy3uuKRhbDx"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "env = gym.make(ENV_NAME, is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8apnz0N_T-W"
   },
   "source": [
    "In this case the observation space has 16 states coded by a disctinct integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1747404034134,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "5H-MvnvWY0bP",
    "outputId": "ee1833a5-21aa-49f5-be80-ad3899c05a4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhYy3sEV_uUC"
   },
   "source": [
    "There are 4 actions,\n",
    "\n",
    "    0: Move left\n",
    "\n",
    "    1: Move down\n",
    "\n",
    "    2: Move right\n",
    "\n",
    "    3: Move up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1747404037928,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "-KDKz9uT_QHt",
    "outputId": "c57b2add-2e4f-431a-e3f4-98106bea45d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747404073711,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "FZhxWjLA2fV-",
    "outputId": "36ff34b8-9ec2-435e-b944-b3d9267ea38f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a random action\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uslx3CWQAOAi"
   },
   "source": [
    "In this case, when we reset the environment we always start in the \"first\" position. in other cases the envoronment can reset at different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747404290025,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "X6lmIlgH_bNC",
    "outputId": "899c30ad-82bc-481c-8d69-46fe776e7257"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs,_ = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc-eDfDM2-5M"
   },
   "source": [
    "Taking an action results in a new-state, a reward and information concerning if we reached the end of the game, either because we've achieved and end state or reached a maximum number of allowed steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747404163384,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "qZkMSMYM2pQT",
    "outputId": "19d57ac1-7c03-4a81-8d7b-a036c7a160aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 False False\n"
     ]
    }
   ],
   "source": [
    "action = 3\n",
    "new_state, reward, is_done, is_trunc, _ = env.step(action)\n",
    "print(new_state, reward, is_done, is_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC9y72md4D1q"
   },
   "source": [
    "Let's create agents that play randomly and collect the corresponding rewards.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1lERIBbTAiPH"
   },
   "outputs": [],
   "source": [
    "class Agent_Random:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, is_done, is_trunc, _ = env.step(action)\n",
    "        self.total_reward += reward\n",
    "        return  new_state, reward, is_done, is_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747404278928,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "ixV2I8Vq5u4J",
    "outputId": "3eb8afd7-6319-494b-bf38-7e876b9302a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 False False\n",
      "0 0.0 False False\n",
      "1 0.0 False False\n",
      "5 0.0 True False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "agent = Agent_Random()\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "  new_state, reward, is_done, is_trunc  = agent.step(env)\n",
    "  print(new_state, reward, is_done, is_trunc)\n",
    "  done = is_done or is_trunc\n",
    "\n",
    "agent.total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVkze7qn67Oo"
   },
   "source": [
    "We'll now visualize our agent in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1747404429841,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "p1SO50c86mJY",
    "outputId": "f8802b22-f8ea-4145-9eb6-43da12e5d450"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimageio\u001b[39;00m\n\u001b[32m      4\u001b[39m env = gym.make(ENV_NAME, render_mode=\u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m, is_slippery=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m obs, _ = env.reset(seed=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import imageio\n",
    "\n",
    "env = gym.make(ENV_NAME, render_mode=\"rgb_array\", is_slippery=False)\n",
    "obs, _ = env.reset(seed=0)\n",
    "done = False\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    frames.append(env.render())\n",
    "    new_state, reward, is_done, is_trunc  = agent.step(env)\n",
    "    print(new_state, reward, is_done, is_trunc)\n",
    "    done =  is_done or is_trunc\n",
    "frames.append(env.render())\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Save to video\n",
    "imageio.mimsave(\"pong_video.mp4\", frames, fps=5)\n",
    "\n",
    "# Show video\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open(\"pong_video.mp4\", 'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=400 controls>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veBCQ2cL8FCi"
   },
   "source": [
    "**Exercise**: Build a class whose agents always choose to go left. And another whose agents choose left 50% of the time and right the other 50%.\n",
    "\n",
    "**Exercise**: Change the environment to \"Taxi-v3\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlvVsQsJ9M8W"
   },
   "source": [
    "To see how an agent can learn an optimal policy, we will very briefly recall the idea of **value function** and the fundamental result captured by **Bellman's equation**.\n",
    "\n",
    "Given a discount factor $\\gamma\\in]0,1]$ and a policy\n",
    "$\\pi:{\\cal S}\\rightarrow {\\cal A}$, we define the associated (state-action) value function to be the function\n",
    "\n",
    "$$Q_{\\pi}:{\\cal S}\\times{\\cal A}\\rightarrow \\mathbb{R}$$\n",
    "\n",
    "such that $Q_{\\pi}(s,a)$ *equals the (expected) discounted reward obtained by choosing action $a$ at state $s$ and then following policy $\\pi$ until the end of the game*.\n",
    "\n",
    "Now consider the value function $Q = Q_{\\pi^*}$ associated with an optimal policy $\\pi^*$. A fundamental mathematical result underlying important achievements in RL is the fact that (the optimal) $Q$ is a solution of Bellman's equation\n",
    "\n",
    "$$ Q(s,a) = R(s,a) + \\max_{a'\\in {\\cal A}} Q(s',a') \\;, $$\n",
    "\n",
    "where $s'$ is the state reached from state $s$, by taking action $a$. Once again, we are simplifying the presentation by only considering the deterministic setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kaug7ZYaC5pT"
   },
   "source": [
    "# Q - tabular learning\n",
    "\n",
    "We'll now describe an implementation of the Q-tabular learning algorithm, following the presentation in Chapter 5 of Lapan's \"Deep Reinforcement Learning Hands On - Third Edition\".\n",
    "\n",
    "\n",
    "1. The idea for this algorithm is to construct a double entry table for the $Q$ values (we'll use a dictionary with keys (state,action) and values equal to the Q-values).  \n",
    "\n",
    "2. In the beginning, our values dictionary is empty because our agent doesn't know anything about the environment.\n",
    "\n",
    "3. To learn about the environment, in each step, the agent performs a random action and records the corresponding transition information in a tuple $(s,a,r,s')=$(state, action, reward, new_state).\n",
    "\n",
    "4. Then it updates the $Q$ value using this new information, by using the rule\n",
    "\n",
    "$$Q(s,a) \\leftarrow (1-\\alpha)Q(s,a) + \\alpha (r + \\gamma\\max_{a'}Q(s',a'))\\;.$$\n",
    "\n",
    "For $\\alpha=1$ the update rule corresponds to a fixed point iteration of Bellman's equation (for which there are very useful convergence results); the inclusion of an $\\alpha\\in ]0,1]$ is useful, in practice, as it provides numerical stability.\n",
    "\n",
    "5. After a fixed number of iterations (TEST_TIME) of the previous two steps, we test how well our agent as learned. This is done by playing a given number (TEST_EPISODES) of complete episodes/games and checking if the average discounted return achieves a fixed threshold (MEAN_REWARD_GOAL). If that is the case, we stop training; otherwise, we return to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sb3EuaVu7zBo"
   },
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict, Counter\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTUDIGigDOwr"
   },
   "outputs": [],
   "source": [
    "State = int\n",
    "Action = int\n",
    "ValuesKey = tt.Tuple[State, Action]\n",
    "\n",
    "class Agent:\n",
    "  def __init__(self, env):\n",
    "      self.env = env\n",
    "      self.state, _ = self.env.reset()\n",
    "      self.values: tt.Dict[ValuesKey, float] = defaultdict(float)\n",
    "\n",
    "\n",
    "  def sample_env(self) -> tt.Tuple[State, Action, float, State]:\n",
    "      action = self.env.action_space.sample()\n",
    "      old_state = self.state\n",
    "      new_state, reward, is_done, is_trunc, _ = self.env.step(action)\n",
    "      if is_done or is_trunc:\n",
    "        self.state, _ = self.env.reset()\n",
    "      else:\n",
    "        self.state = new_state\n",
    "      return old_state, action, float(reward), new_state\n",
    "\n",
    "  def best_value_and_action(self, state: State) -> tt.Tuple[float, State]:\n",
    "    best_action, best_value = None, None\n",
    "    for action in range(self.env.action_space.n):\n",
    "      action_value = self.values[(state, action)]\n",
    "      if best_value is None or best_value < action_value:\n",
    "          best_value = action_value\n",
    "          best_action = action\n",
    "    return best_value, best_action\n",
    "\n",
    "  def value_update(self, state: State, action: Action, reward: float, next_state: State):\n",
    "      best_value, _ = self.best_value_and_action(next_state)\n",
    "      new_value = reward + GAMMA*best_value\n",
    "      old_value = self.values[(state, action)]\n",
    "      self.values[(state, action)] = (1-ALPHA)*old_value + ALPHA*new_value\n",
    "\n",
    "  def play_episode(self, env: gym.Env) -> float:\n",
    "    total_reward = 0.0\n",
    "    state, _ = env.reset()\n",
    "    while True:\n",
    "        _ , best_action = self.best_value_and_action(state)\n",
    "        new_state, reward, is_done, is_trunc, _ = env.step(best_action)\n",
    "        total_reward += reward\n",
    "        if is_done or is_trunc:\n",
    "          break\n",
    "        state = new_state\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11296,
     "status": "ok",
     "timestamp": 1747409853719,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "bZ_pdtnPDZLh",
    "outputId": "4884a8e5-f110-41e7-daea-f54c1183fc35"
   },
   "outputs": [],
   "source": [
    "# If ENV_NAME is \"FrozenLake-v1\", add \"is_slippery = False\" ...\n",
    "#... to env constructor to obtain a deterministic game.\n",
    "\n",
    "ENV_NAME =   \"FrozenLake-v1\" # \"Taxi-v3\"\n",
    "env = gym.make(ENV_NAME), is_slippery = False)\n",
    "test_env = gym.make(ENV_NAME), is_slippery = False)\n",
    "agent = Agent(env)\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 20\n",
    "MEAN_REWARD_GOAL = .8\n",
    "TEST_TIME = 3000\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "while True:\n",
    "  iter_no += 1\n",
    "  state, action, reward, next_state = agent.sample_env()\n",
    "  agent.value_update(state, action, reward, next_state)\n",
    "  if iter_no % TEST_TIME == 0:\n",
    "    print(f\"iter_no={iter_no}\")\n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):\n",
    "      reward += agent.play_episode(test_env)\n",
    "    reward /= TEST_EPISODES\n",
    "    if reward > best_reward:\n",
    "        print(f\"{iter_no}: Best reward updated {best_reward:.3} -> {reward:.3}\")\n",
    "        best_reward = reward\n",
    "    if reward > MEAN_REWARD_GOAL:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVRvOOEbECQ6"
   },
   "source": [
    "Let's see our optimal agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1747409948943,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "KzmX7_aEEOV5",
    "outputId": "6c3b8d65-eefb-4e54-f2bd-d25420f1d725"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import imageio\n",
    "\n",
    "env = gym.make(ENV_NAME, render_mode=\"rgb_array\", is_slippery = False)\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    frames.append(env.render())\n",
    "    #\n",
    "    best_value, best_action = agent.best_value_and_action(state)\n",
    "    new_state, reward, is_done, is_trunc, _ = env.step(best_action)\n",
    "    #\n",
    "    # to compare with random agent, comment the previous 2 lines and uncomment the next 2\n",
    "    #\n",
    "    #random_action = env.action_space.sample()\n",
    "    #new_state, reward, is_done, is_trunc, _ = env.step(random_action)\n",
    "    #\n",
    "    done =  is_done or is_trunc\n",
    "    state = new_state\n",
    "frames.append(env.render())\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Save to video\n",
    "imageio.mimsave(\"pong_video.mp4\", frames, fps=5)\n",
    "\n",
    "# Show video\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open(\"pong_video.mp4\", 'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=400 controls>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_yxCfjvnS_I"
   },
   "source": [
    "# Deep Q Learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9p2ibhhp-ta"
   },
   "source": [
    "If the number of states becomes too large, tabular learning becomes unfeasible. A way to circumvent this problem is to model $Q$ as a deep neural network that receives as inputs the state and the action, and outputs the corresponding $Q$ value. The parameters of this network can then be learned via gradient descent by considering a loss that measures how far $Q$ is from satisfying Bellman's equation.\n",
    "\n",
    "This is the basic conceptual idea behind the major breakthrough made in 2013 by a team from DeepMind https://arxiv.org/abs/1312.5602, which allowed them to train a DQN (Deep Q Network) to play several Atari games; in fact, the networke learned \"by itself\" starting from the pixel data of the frames of the computer games. This remarkable breakthrough was the starting point to a spectacular series of achievements that included mastering high-complexity strategy games such as chess and Go, and most importantly, the computation of the 3D structure of proteins https://www.nobelprize.org/prizes/chemistry/2024/summary/.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AappnB_LtXdi"
   },
   "source": [
    "A naive version of the DQN algorithm allude to above, can be summarizes as follows:\n",
    "\n",
    "1. Initialize $Q(s, a)$ using a standard deep learning initialization.\n",
    "\n",
    "2. By interacting with the environment, obtain the tuple $(s, a, r, s')$.\n",
    "\n",
    "3. Compute the loss:\n",
    "\n",
    "$${\\cal L} =\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    " (Q(s, a) - r)^2  \\quad,\\text{ if episode ended}\\;, \\\\\n",
    " (Q(s, a) - (r + \\gamma \\max_{a'} Q(s' ,a'))^2 \\quad, \\text{ otherwise} \\;.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "4. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
    "\n",
    "5. Repeat from step 2 until convergence (in the sense described for tabular-$Q$ learning) is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCs21Aj2b50h"
   },
   "source": [
    "We'll now focus on a single Atari game, the iconic Pong https://ale.farama.org/environments/pong/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 101653,
     "status": "ok",
     "timestamp": 1747589553167,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "PhOeajsRcnXZ",
    "outputId": "da34f50d-6001-4f53-8d5b-17f8a3ccb177"
   },
   "outputs": [],
   "source": [
    "!pip install 'stable_baselines3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1747589719453,
     "user": {
      "displayName": "João Costa",
      "userId": "03431830090646328815"
     },
     "user_tz": -60
    },
    "id": "MZeLCmSQb5IR",
    "outputId": "f41ed308-a128-45c7-c3e0-9fc2f51929a0"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common import atari_wrappers\n",
    "import ale_py\n",
    "\n",
    "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "\n",
    "obs,_ = env.reset()\n",
    "\n",
    "print(obs.shape)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbSoytnibWas"
   },
   "source": [
    "Unfortunately, the previous version of DQN doesn't work very well. So we need to do perform some upgrades.\n",
    "\n",
    "- On one hand, we need to explore the environment (using random actions); on the other, we want to use the knowledge gained by the $Q$-function; this is the famous \"exploration versus exploitation dilemma\".  We will resolve this dilemma by introducing a parameter $\\epsilon\\in[0,1]$, that will decrease with the number of iterations, and will be used to decide, with probability $\\epsilon$, if the agent will take a random action or will choose the action prescribed by $Q$.\n",
    "\n",
    "- We will also introduce a **replay buffer**. This will store a \"large\" number of transitions $(s, a, r, s')$ that will be used to construct training data batches to update the parameters of $Q$ using gradient descent.\n",
    "\n",
    "- Finally, using $Q$ itself to generate the targets for the loss will make the training very unstable. To circumvent this problem, we will introduce another DQN $\\hat Q$, called the **target network**, that is periodically synchronized with the main $Q$ network, but otherwise remains unchanged for a given number of iterations.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zfjMdN1OO5S"
   },
   "source": [
    "Upgraded DQN algorithm:  \n",
    "\n",
    "1. Initialize parameters for $Q(s,a)$ and $\\hat Q(s,a)$,$\\;\\epsilon \\leftarrow 1.0$, and empty the replay buffer.\n",
    "\n",
    "2. With probability $\\epsilon$, select a random action $a$; otherwise, $a = \\text{argmax}_a Q(s, a)$.\n",
    "\n",
    "3. Execute action $a$ in an emulator and observe the reward, $r$, and the next state, $s'$.\n",
    "\n",
    "4. Store the transition $(s, a, r, s')$ in the replay buffer.\n",
    "\n",
    "5. Sample a random mini-batch of transitions from the replay buffer.\n",
    "\n",
    "6. For every transition in the buffer, calculate the target:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    " r  \\quad,\\text{ if episode ended}\\;, \\\\\n",
    " r + \\gamma \\max_{a'} Q(s' ,a') \\quad, \\text{ otherwise} \\;.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "7. Calculatetheloss: ${\\cal L}=(Q(s,a)-y)^2.\n",
    "\n",
    "8. Update $Q(s, a)$ using an appropriate gradient descent based algorithm, to minimize the loss with respect to the $Q$ model parameters.\n",
    "\n",
    "9. Repeat from step 2 until convergence (in the sense described before) is achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7UB14l5VB7d"
   },
   "source": [
    "We will now present the Lapan's implementation of this algorithm, as exposed in chapter 6 of his book; see also\n",
    "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition.\n",
    "\n",
    "Note that Lapan uses PyTorch to implement the DQNs. It is a quite instructive exercise to translate the code to tensorflow/keras.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpamMumdWN-r"
   },
   "source": [
    "# **Wrappers:** For efficiency and conceptual reasons, we need to preprocess the environment. For instance:\n",
    "\n",
    "- We can reduce the game frames by considering only a monochromatic version of them and a lower resolution. This is \"hidden\" in the *atari_wrappers.AtariWrapper* class. After applying this, our images will have a shape of $(84,84,1)$; recall that originally they had a shape equal to $(210,160,3)$. Note that this class takes care of a lot of more relevant preprocessing details (see Chapter 6 of Lapan's book for more details).   \n",
    "\n",
    "- This isn't the way PyTorch is designed to receive information; it expects the form (channels, height, width). The ImageToPyTorch wrapper, presented below, takes care of this.\n",
    "\n",
    "- The agent won't be able to learn how to play Pong if we only provide still images of the game. To learn how to play Pong, we need to learn about dynamics. So we need to pack a given number (n_steps) of consecutive images into a \"small video\" with n_steps frames. This is taken care of by the BufferWrapper (see below).\n",
    "\n",
    "Here is the code for the wrappers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlM3EYsxZFQg"
   },
   "outputs": [],
   "source": [
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        obs = self.observation_space\n",
    "        assert isinstance(obs, gym.spaces.Box)\n",
    "        assert len(obs.shape) == 3\n",
    "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=obs.low.min(), high=obs.high.max(),\n",
    "            shape=new_shape, dtype=obs.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        obs = env.observation_space\n",
    "        assert isinstance(obs, spaces.Box)\n",
    "        new_obs = gym.spaces.Box(\n",
    "            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
    "            dtype=obs.dtype)\n",
    "        self.observation_space = new_obs\n",
    "        self.buffer = collections.deque(maxlen=n_steps)\n",
    "\n",
    "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
    "        for _ in range(self.buffer.maxlen-1):\n",
    "            self.buffer.append(self.env.observation_space.low)\n",
    "        obs, extra = self.env.reset()\n",
    "        return self.observation(obs), extra\n",
    "\n",
    "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
    "        self.buffer.append(observation)\n",
    "        return np.concatenate(self.buffer)\n",
    "\n",
    "\n",
    "def make_env(env_name: str, **kwargs):\n",
    "    env = gym.make(env_name, **kwargs)\n",
    "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, n_steps=4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmXLwfJHZO-X"
   },
   "source": [
    "For the DQN model, we will use a typical convolution network, with a conv base followed by a dense network. Importantly, instead of modeling $Q$ as a function of the pair $(s,a)$ that outputs the corresponding value, i.e.,\n",
    "\n",
    "$$Q: {\\cal S}\\times {\\cal A} \\rightarrow \\mathbb{R}\\;,$$\n",
    "\n",
    "where, for our environment, we have\n",
    "\n",
    "$${\\cal S}=\\mathbb{R}^{84\\times 84\\times 4}$$\n",
    "\n",
    "and\n",
    "\n",
    "$${\\cal A} =\\{0,1,2,3,4,5\\} \\subset \\mathbb{R}\\;,$$\n",
    "\n",
    "we will use a dual representation   \n",
    "\n",
    "$$Q: {\\cal S} \\rightarrow \\mathbb{R}^6\\;,$$\n",
    "\n",
    "that given the state, outputs the value for each possible action.\n",
    "\n",
    "The PyTorch code should be, by now, self-explanatory. Notice nonetheless that it requires a bit more work, than the one needed in Keras to set dimensions of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6GBp08adVU0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.ByteTensor):\n",
    "        # scale on GPU\n",
    "        xx = x / 255.0\n",
    "        return self.fc(self.conv(xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTFQhRa3ddnF"
   },
   "source": [
    "Next we define several parameters and variables and construct the class fro replay buffer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aG1OXGH_dvxj"
   },
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 150000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.01\n",
    "\n",
    "State = np.ndarray\n",
    "Action = int\n",
    "BatchTensors = tt.Tuple[\n",
    "    torch.ByteTensor,           # current state\n",
    "    torch.LongTensor,           # actions\n",
    "    torch.Tensor,               # rewards\n",
    "    torch.BoolTensor,           # done || trunc\n",
    "    torch.ByteTensor            # next state\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class Experience:\n",
    "    state: State\n",
    "    action: Action\n",
    "    reward: float\n",
    "    done_trunc: bool\n",
    "    new_state: State\n",
    "\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience: Experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
    "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
    "        return [self.buffer[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MNCg8vueHfi"
   },
   "source": [
    "We now create the agent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zknt1frZeSeI"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self.state: tt.Optional[np.ndarray] = None\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state, _ = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net: dqn_model.DQN, device: torch.device,\n",
    "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_v = torch.as_tensor(self.state).to(device)\n",
    "            state_v.unsqueeze_(0)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(\n",
    "            state=self.state, action=action, reward=float(reward),\n",
    "            done_trunc=is_done or is_tr, new_state=new_state\n",
    "        )\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done or is_tr:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6pMmUrjeTzi"
   },
   "source": [
    "We will also need to transform our samples taken from the buffer into tensors that can be fed to our networks. This is achieved by this simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSLXGUYWeWYH"
   },
   "outputs": [],
   "source": [
    "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
    "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
    "    for e in batch:\n",
    "        states.append(e.state)\n",
    "        actions.append(e.action)\n",
    "        rewards.append(e.reward)\n",
    "        dones.append(e.done_trunc)\n",
    "        new_state.append(e.new_state)\n",
    "    states_t = torch.as_tensor(np.asarray(states))\n",
    "    actions_t = torch.LongTensor(actions)\n",
    "    rewards_t = torch.FloatTensor(rewards)\n",
    "    dones_t = torch.BoolTensor(dones)\n",
    "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
    "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
    "           dones_t.to(device),  new_states_t.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spsqJAktfDU9"
   },
   "source": [
    "A not-so-easy piece of code, even being quite small, is the one to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwIK6i2ifNMj"
   },
   "outputs": [],
   "source": [
    "def calc_loss(batch: tt.List[Experience], net: dqn_model.DQN, tgt_net: dqn_model.DQN,\n",
    "              device: torch.device) -> torch.Tensor:\n",
    "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
    "\n",
    "    state_action_values = net(states_t).gather(\n",
    "        1, actions_t.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = tgt_net(new_states_t).max(1)[0]\n",
    "        next_state_values[dones_t] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_t\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84nZVAyFgCw6"
   },
   "source": [
    "Finally, we have the code for the training loop. Note that this training procedure is quite expensive it will be unfeaseble without the access to a GPU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-9QeKX_f13p"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
    "parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
    "                    help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
    "args = parser.parse_args()\n",
    "device = torch.device(args.dev)\n",
    "\n",
    "env = wrappers.make_env(args.env)\n",
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-\" + args.env)\n",
    "print(net)\n",
    "\n",
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_m_reward = None\n",
    "\n",
    "\n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "    reward = agent.play_step(net, device, epsilon)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        m_reward = np.mean(total_rewards[-100:])\n",
    "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
    "              f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "        if best_m_reward is None or best_m_reward < m_reward:\n",
    "            torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
    "            if best_m_reward is not None:\n",
    "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
    "            best_m_reward = m_reward\n",
    "        if m_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "    if frame_idx % 10 == 0:\n",
    "        print(f\"training in frame {frame_idx}, Loss = {loss_t:.4f}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgrTii2EhdQZ"
   },
   "source": [
    "The evolution of our agent's ability to play pong is recorded in the following collection of videos\n",
    "https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NdwRG-rhvzl"
   },
   "source": [
    "# Worksheet #9: Rewrite Laplan's code using TensorFlow and Keras."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQynSOmti3szwbJuKNykc5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
